{"./":{"url":"./","title":"软件中心","keywords":"","body":"软件中心 目录 一、软件下载 1.python软件 2.数据库相关 3.linux系统 4.逆向相关 其它 二、常用pip源 三、国内知名镜像源 一、软件下载 1.python软件 pycharm：官网 python：官网 Anaconda：清华镜像站点 2.数据库相关 mysql5.7：清华大学镜像站 与 官网下载 mongo：官网下载 redis 官网下载：https://redis.io/download Navicat： studio：暂无 3.linux系统 vmware： centos系统：MSDN mobaXterm：官网 4.逆向相关 nodejs：官网 js调试助手： 抓包工具Fiddler： postman： 其它 xmind： vncviewer：官网 万彩办公大师： git：github中下载 喧喧：官网下载 docker： 见以下国内知名镜像源 二、常用pip源 阿里云 http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 豆瓣(douban) http://pypi.douban.com/simple/ 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ 中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/ 三、国内知名镜像源 清华大学开源软件镜像站 北京外国语大学开源软件镜像站 上海交通大学开源软件镜像站 中国科技大学开源镜像站 "},"part1_jskj/":{"url":"part1_jskj/","title":"一、服务器应用","keywords":"","body":"稷生科技 已更新 稷生人员表 服务器端口介绍(未完) 行业数据库对应表 国情数据库对应表 阿波罗配置文件 待更新 mysql对应表 其它文件 员工请假申请表：下载pdf 离职证明：下载pdf 打印机驱动： 下载rar 国民经济分类：下载pdf "},"part1_jskj/server_addr.html":{"url":"part1_jskj/server_addr.html","title":"服务器端口","keywords":"","body":"服务器服务 序号 项目名 url 说明 1 Jenkins http://192.168.0.11:8088/ 代码自动部署 2 gitlab http://192.168.0.11 代码仓库 3 Apollo http://192.168.3.85:8070 动态配置管理 4 scrapydweb http://192.168.3.85:5000 爬虫监控中心 5 proxypool http://192.168.3.85:5010 代理池服务 6 transfile http://192.168.3.85:8500/file/upload/ 文件上传服务 7 xxl-job http://192.168.0.11:8066/doc.html 数据定时插入 scrapy服务 序号 负责人 服务端口 1 郭俊杰 http://192.168.3.85:6808 2 钱兆烺 http://192.168.3.85:6810 3 朱家井 http://192.168.3.85:6811 4 郑宇飞 http://192.168.3.85:6815 5 郑宇飞 http://192.168.3.85:6816 6 王国锋 http://192.168.3.85:6820 7 程厚富 http://192.168.3.85:6821 8 李子言 http://192.168.3.85:6822 9 钟振坤 http://192.168.3.85:6823 10 彭帅 http://192.168.3.85:6824 10 陈志霖 http://192.168.3.85:6825 11 邓俊 http://192.168.3.85:6826 12 吴健爱 http://192.168.3.85:6830 "},"part1_jskj/team_member.html":{"url":"part1_jskj/team_member.html","title":"团队人员","keywords":"","body":"jskj 团队人员信息 编号（string） 名字 性别 职责 备注 01 朱家井 1 爬虫 已离职 02 李阿兵 1 爬虫 03 郭俊杰 1 爬虫 04 郑宇飞 1 爬虫 05 陈志霖 1 爬虫 06 彭帅 1 爬虫 已离职 07 钱兆烺 1 爬虫 已离职 08 王国锋 1 爬虫 已离职 09 卢锦威 1 爬虫 实习结束 10 曹金鹏 1 爬虫 实习结束 11 邓俊 1 爬虫 12 刘珊杉 0 数据清洗 13 吴健爱 0 爬虫 14 彭思祺 1 后端 16 丁立冯 0 报告研究 已离职 17 罗枳珊 0 前端 已离职 18 钟振坤 1 爬虫 实习结束 19 程厚富 1 爬虫 20 黄伟皓 1 爬虫 "},"part1_jskj/mongo_industry.html":{"url":"part1_jskj/mongo_industry.html","title":"行业mongo数据库","keywords":"","body":" 序号 姓名 行业 1 陈志霖 chenzhilin_A03_data 2 陈志霖 chenzhilin_A03_info 3 陈志霖 chenzhilin_A03_repo 4 陈志霖 chenzhilin_C29_data 5 陈志霖 chenzhilin_C29_info 6 陈志霖 chenzhilin_C29_repo 7 陈志霖 chenzhilin_C31_data 8 陈志霖 chenzhilin_C31_info 9 陈志霖 chenzhilin_C31_repo 10 陈志霖 chenzhilin_C37_data 11 陈志霖 chenzhilin_C37_info 12 陈志霖 chenzhilin_C37_repo 13 陈志霖 chenzhilin_C39_data 14 陈志霖 chenzhilin_C39_info 15 陈志霖 chenzhilin_C39_repo 16 陈志霖 chenzhilin_G59_data 17 陈志霖 chenzhilin_G59_info 18 陈志霖 chenzhilin_G59_repo 19 陈志霖 chenzhilin_I65_data 20 陈志霖 chenzhilin_I65_info 21 陈志霖 chenzhilin_I65_repo 22 程厚富 chf_D44_data 23 程厚富 chf_D44_info 24 程厚富 chf_D44_repo 25 程厚富 chf_E_data 26 程厚富 chf_E_info 27 程厚富 chf_E_repo 28 邓俊 dj_C14_data 29 邓俊 dj_C14_info 30 邓俊 dj_C14_repo 31 邓俊 dj_C27_data 32 邓俊 dj_C27_info 33 邓俊 dj_C27_repo 34 邓俊 dj_C42_data 35 邓俊 dj_C42_info 36 邓俊 dj_H61_data 37 邓俊 dj_H61_info 38 邓俊 dj_H61_repo 39 邓俊 dj_K70_data 40 邓俊 dj_K70_info 41 邓俊 dj_K70_repo 42 邓俊 dj_O80_data 43 邓俊 dj_O80_info 44 邓俊 dj_O81_data 45 邓俊 dj_O81_info 46 邓俊 dj_jiaoyu_data 47 郭俊杰 guojunjie_A02_data 48 郭俊杰 guojunjie_A02_info 49 郭俊杰 guojunjie_A02_report 50 郭俊杰 guojunjie_C20_data 51 郭俊杰 guojunjie_C20_info 52 郭俊杰 guojunjie_C20_repo 53 郭俊杰 guojunjie_F51_data 54 郭俊杰 guojunjie_F51_info 55 郭俊杰 guojunjie_F51_repo 56 郭俊杰 guojunjie_F52_data 57 郭俊杰 guojunjie_F52_info 58 郭俊杰 guojunjie_F52_repo 59 郭俊杰 guojunjie_F52_report 60 郭俊杰 guojunjie_R87_data 61 郭俊杰 guojunjie_R87_info 62 郭俊杰 guojunjie_R87_repo 63 郭俊杰 guojunjie_R89_data 64 郭俊杰 guojunjie_R89_info 65 郭俊杰 guojunjie_R89_repo 66 郭俊杰 guojunjie_R90_data 67 郭俊杰 guojunjie_R90_info 68 郭俊杰 guojunjie_R90_repo 69 李子言 lab_A04_repo 70 李子言 lab_B07_data 71 李子言 lab_B07_info 72 李子言 lab_B07_repo 73 李子言 lab_B25_info 74 李子言 lab_C25_data 75 李子言 lab_C25_info 76 李子言 lab_C25_repo 77 李子言 lab_D45_data 78 李子言 lab_D45_info 79 李子言 lab_G54_data 80 李子言 lab_G54_info 81 李子言 lab_G54_repo 82 李子言 lab_G58_data 83 李子言 lab_G58_info 84 李子言 lab_G58_repo 85 曹金鹏 cjp_B06_data 86 曹金鹏 cjp_B06_info 87 卢锦威 ljw_A04_data 88 卢锦威 ljw_A04_info 89 彭帅 ps_B08_data 90 彭帅 ps_B08_info 91 彭帅 ps_C21_data 92 彭帅 ps_C21_info 93 彭帅 ps_C21_repo 94 彭帅 ps_C23_data 95 彭帅 ps_C23_info 96 彭帅 ps_C23_repo 97 彭帅 ps_C28_data 98 彭帅 ps_C28_info 99 彭帅 ps_C40_data 100 彭帅 ps_C40_info 101 彭帅 ps_C40_repo 102 彭帅 ps_G57_data 103 彭帅 ps_G57_info 104 彭帅 ps_G57_repo 105 彭帅 ps_R88_data 106 彭帅 ps_R88_info 107 彭帅 ps_info 108 钱兆烺 qzl_c15_data 109 钱兆烺 qzl_c15_data1 110 钱兆烺 qzl_c15_info 111 钱兆烺 qzl_c15_repo 112 钱兆烺 qzl_c24_data 113 钱兆烺 qzl_c24_info 114 钱兆烺 qzl_c24_repo 115 钱兆烺 qzl_c38_data 116 钱兆烺 qzl_c38_info 117 钱兆烺 qzl_c38_repo 118 钱兆烺 qzl_m73_data 119 钱兆烺 qzl_m73_info 120 钱兆烺 qzl_m74_data 121 钱兆烺 qzl_m74_info 122 钱兆烺 qzl_m74_repo 123 钱兆烺 qzl_m75_data 124 钱兆烺 qzl_m75_info 125 钱兆烺 qzl_m75_repo 126 钱兆烺 qzl_n76_data 127 钱兆烺 qzl_n76_info 128 钱兆烺 qzl_n76_repo 129 王国锋 wgf_B09_data 130 王国锋 wgf_B09_info 131 王国锋 wgf_C32_data 132 王国锋 wgf_C32_info 133 王国锋 wgf_D46_data 134 王国锋 wgf_D46_info 135 王国锋 wgf_D46_repo 136 王国锋 wgf_G56_data 137 王国锋 wgf_G56_info 138 王国锋 wgf_G56_repo 139 王国锋 wgf_G60_data 140 王国锋 wgf_G60_info 141 王国锋 wgf_N77_data 142 王国锋 wgf_N77_info 143 王国锋 wgf_N77_repo 144 王国锋 wgf_N79_data 145 王国锋 wgf_N79_info 146 王国锋 wgf_N79_repo 147 吴健爱 wja_C16_data 148 吴健爱 wja_C16_info 149 吴健爱 wja_C16_repo 150 吴健爱 wja_C19_data 151 吴健爱 wja_C19_info 152 吴健爱 wja_C19_repo 153 吴健爱 wja_C22_data 154 吴健爱 wja_C22_info 155 吴健爱 wja_C22_repo 156 吴健爱 wja_C30_data 157 吴健爱 wja_C30_info 158 吴健爱 wja_C30_repo 159 吴健爱 wja_G55_data 160 吴健爱 wja_G55_info 161 吴健爱 wja_G55_repo 162 吴健爱 wja_L71_data 163 吴健爱 wja_L71_info 164 吴健爱 wja_L71_repo 165 吴健爱 wja_R86_data 166 吴健爱 wja_R86_info 167 吴健爱 wja_R86_repo 168 朱家井 zjj_A01_data 169 朱家井 zjj_A01_info 170 朱家井 zjj_A01_repo 171 朱家井 zjj_C17_data 172 朱家井 zjj_C17_info 173 朱家井 zjj_C17_repo 174 朱家井 zjj_C18_info 175 朱家井 zjj_C18_repo 176 朱家井 zjj_C34_data 177 朱家井 zjj_C34_info 178 朱家井 zjj_C34_repo 179 朱家井 zjj_C35_data 180 朱家井 zjj_C35_info 181 朱家井 zjj_C35_repo 182 朱家井 zjj_G53_data 183 朱家井 zjj_G53_info 184 朱家井 zjj_G53_repo 185 朱家井 zjj_N78_data 186 朱家井 zjj_N78_info 187 朱家井 zjj_N78_repo 188 郑宇飞 zyf_B10_data 189 郑宇飞 zyf_B10_info 190 郑宇飞 zyf_B10_repo 191 郑宇飞 zyf_C13_data 192 郑宇飞 zyf_C13_info 193 郑宇飞 zyf_C13_repo 194 郑宇飞 zyf_C33_data 195 郑宇飞 zyf_C33_info 196 郑宇飞 zyf_C33_repo 197 郑宇飞 zyf_C36_data 198 郑宇飞 zyf_C36_info 199 郑宇飞 zyf_C36_repo 200 郑宇飞 zyf_P83_data 201 郑宇飞 zyf_P83_info 202 郑宇飞 zyf_P83_repo 203 郑宇飞 zyf_Q84_data 204 郑宇飞 zyf_Q84_info 205 郑宇飞 zyf_Q84_repo 206 郑宇飞 zyf_Q85_data 207 郑宇飞 zyf_Q85_info 208 钟振坤 zzk_C26_data 209 钟振坤 zzk_C26_data_pass 210 钟振坤 zzk_C26_info 211 钟振坤 zzk_C26_repo 212 钟振坤 zzk_C26_repo_pass 213 钟振坤 zzk_C43_data 214 钟振坤 zzk_C43_data_pass 215 钟振坤 zzk_C43_info 216 钟振坤 zzk_C43_repo 217 钟振坤 zzk_C43_repo_pass 218 钟振坤 zzk_I63_data 219 钟振坤 zzk_I63_data_pass 220 钟振坤 zzk_I63_info 221 钟振坤 zzk_I63_repo 222 钟振坤 zzk_I63_repo_pass 223 钟振坤 zzk_I64_data 224 钟振坤 zzk_I64_data_pass 225 钟振坤 zzk_I64_info 226 钟振坤 zzk_I64_repo 227 钟振坤 zzk_I64_repo_pass 228 钟振坤 zzk_L72_data 229 钟振坤 zzk_L72_data_pass 230 钟振坤 zzk_L72_info 231 钟振坤 zzk_L72_repo 232 钟振坤 zzk_L72_repo_pass "},"part1_jskj/mongo_nation.html":{"url":"part1_jskj/mongo_nation.html","title":"国情mongo数据库","keywords":"","body":" 序号 姓名 国情 1 陈志霖 chenzhilin_CaiSui_info 2 陈志霖 chenzhilin_Folk_info 3 陈志霖 chenzhilin_Grassland_data 4 陈志霖 chenzhilin_Grassland_repo 5 陈志霖 chenzhilin_Language_info 6 陈志霖 chenzhilin_Population_data 7 陈志霖 chenzhilin_Population_repo 8 陈志霖 chenzhilin_PricePolicy_info 9 陈志霖 chenzhilin_ProfitOfCompany_data 10 陈志霖 chenzhilin_QinDynasty_info 11 陈志霖 chenzhilin_SuiDynasty_info 12 陈志霖 chenzhilin_WorldGDP_data 13 陈志霖 chenzhilin_YuanDynasty_info 14 程厚富 chf_CaiZheng_data 15 程厚富 chf_CaiZheng_info 16 程厚富 chf_Cultural_data 17 程厚富 chf_Cultural_info 18 程厚富 chf_Cultural_repo 19 程厚富 chf_Environment_info 20 程厚富 chf_GlobalCompetitiveness_repo 21 程厚富 chf_InternationalEconomic_info 22 程厚富 chf_Literature_info 23 程厚富 chf_OverallBalance_info 24 程厚富 chf_Philosophy_info 25 程厚富 chf_Price_data 26 程厚富 chf_Price_repo 27 程厚富 chf_historyhan_info 28 邓俊 dj_chanyezc_info 29 邓俊 dj_jiaoyu_data 30 邓俊 dj_jiaoyu_info 31 邓俊 dj_jiaoyu_repo 32 邓俊 dj_jinjianquantixi_data 33 邓俊 dj_jinrongzhibiao_data 34 邓俊 dj_mingguo_info 35 邓俊 dj_nongye_data 36 邓俊 dj_nongye_repo 37 邓俊 dj_taikong_data 38 邓俊 dj_wenxue_info 39 邓俊 dj_yuedujiance_data 40 郭俊杰 gjj_air_data 41 郭俊杰 gjj_chunqiuZG_info 42 郭俊杰 gjj_medicalReso_data 43 郭俊杰 gjj_shangchao_info 44 郭俊杰 gjj_water_data 45 郭俊杰 gjj_water_repo 46 郭俊杰 gjj_xiachao_info 47 郭俊杰 gjj_xiqu_info 48 郭俊杰 gjj_yinyue_info 49 李子言 lab_energy_data 50 李子言 lab_energy_repo 51 李子言 lab_gonghe_info 52 李子言 lab_ming_info 53 李子言 lab_quanmou_info 54 李子言 lab_sifa_data 55 李子言 lab_sourceManage_info 56 李子言 lab_wudai_info 57 彭帅 ps_BusinessFlow_info 58 彭帅 ps_Businessflow_info 59 彭帅 ps_Forest03_repo 60 彭帅 ps_Forest_data 61 彭帅 ps_Forest_repo 62 彭帅 ps_Touzi_data 63 彭帅 ps_Touzi_info 64 彭帅 ps_World_data 65 彭帅 ps_jingguan_data 66 彭帅 ps_jingguan_info 67 彭帅 ps_jiyi_info 68 彭帅 ps_lvyou_data 69 彭帅 ps_qingchao_info 70 彭帅 ps_wushu_info 71 钱兆烺 qzl_shengwuziyuan_data 72 钱兆烺 qzl_shengwuziyuan_repo 73 王国锋 wgf_Ancient_Civilization_info 74 王国锋 wgf_Jin_dynasties_info 75 王国锋 wgf_Shidi_data 76 王国锋 wgf_Shidi_repo 77 王国锋 wgf_Tech_Resource_data 78 王国锋 wgf_Tech_Resource_repo 79 王国锋 wgf_Zhou_dynasty_info 80 吴健爱 wja_ChinamoneyCR_list_data 81 吴健爱 wja_CompanyCR_list_data 82 吴健爱 wja_CountryCR_list_data 83 吴健爱 wja_Income_Distribution_info 84 吴健爱 wja_fengjing_info 85 吴健爱 wja_gjzy_data 86 吴健爱 wja_listedcompany_data 87 吴健爱 wja_listedcompany_list_data 88 吴健爱 wja_mineral_data 89 吴健爱 wja_mineral_repo 90 吴健爱 wja_power_data 91 吴健爱 wja_power_repo 92 吴健爱 wja_quyujingji_info 93 吴健爱 wja_songchao_info 94 吴健爱 wja_xfzc_info 95 吴健爱 wja_xfzc_info_old 96 吴健爱 wja_yinshi_info 97 朱家井 zjj_gdp_data 98 朱家井 zjj_ocean_data 99 朱家井 zjj_ocean_repo 100 朱家井 zjj_study_data 101 朱家井 zjj_study_repo 102 朱家井 zjj_threeking_info 103 朱家井 zjj_trade_data 104 朱家井 zjj_wanqi_info 105 朱家井 zjj_wanqi_info_old 106 朱家井 zjj_xinglv_info 107 郑宇飞 zyf_Architecture_info 108 郑宇飞 zyf_CaiJing_Policy_info 109 郑宇飞 zyf_Env_Indic_data 110 郑宇飞 zyf_Finance_Policy_info 111 郑宇飞 zyf_Finance_data 112 郑宇飞 zyf_IEI_data 113 郑宇飞 zyf_IFS_data 114 郑宇飞 zyf_IFS_info 115 郑宇飞 zyf_InfoRes_data 116 郑宇飞 zyf_InfoRes_repo 117 郑宇飞 zyf_Investment_Policy_info 118 郑宇飞 zyf_LandRes_data 119 郑宇飞 zyf_LandRes_info 120 郑宇飞 zyf_LandRes_repo 121 郑宇飞 zyf_Medicine_info 122 钟振坤 zzk_Caigou_data 123 钟振坤 zzk_Caigou_data_pass 124 钟振坤 zzk_Fushi_info 125 钟振坤 zzk_Jiaotong_data 126 钟振坤 zzk_Jiaotong_data_pass 127 钟振坤 zzk_Jiaotong_info 128 钟振坤 zzk_Jiaotong_repo 129 钟振坤 zzk_Jingjitizhi_info 130 钟振坤 zzk_Sky_data 131 钟振坤 zzk_Tangchao_info 132 钟振坤 zzk_Wenzi_info 133 钟振坤 zzk_Xingzheng_data 134 钟振坤 zzk_Xingzheng_data_pass "},"part1_jskj/hot_industry_field.html":{"url":"part1_jskj/hot_industry_field.html","title":"热门行业字段","keywords":"","body":"热门行业字典目录 资讯字段 数据字段 报告字段 热门行业资讯字段 序号 字段 类型 名称 备注 1 news_id string id url哈希值 2 category string 行业 中文 3 sub_category string 行业子类 中文 4 information_categories string 资讯类别 5 content_url string 链接地址 6 title string 标题 7 issue_time string 发布时间 8 title_image string 标题图片 9 information_source string 网站名 10 source string非list 来源 11 author string非list 作者 12 content string非list 内容 13 images string非list 图片 14 attachments string非list 附件 15 area string 地区 16 address string 地址 17 tags string非list 标签 18 sign string sign 02 19 update_time string 时间戳 str(int(time.time()*1000)) 20 cleaning_status int 清洗位 默认为0 热门行业数据字段 字段 类型 解释 示例 parent_id string 数据目录 1 001 001 001 indic_name string 名称 data_year int 年：1992 data_month int 月：1-12 没有为0 data_day int 日：1-31 没有为0 frequency int 频率(0：季度， 1234： 季度 ，5678：年月周日 ) 1 unit string 单位 data_source string 数据来源(网站名) 国家统计局 region string 全国、省份、市等地区 country string 国家 create_time date 数据产生时间 xxxx-xx-xx update_time datetime 数据插入时间（爬取时间） data_value double 数值 sign string 个人编号 01-20 status int 0:无效 1: 有效 cleaning_status int 0 : 未清洗 1 ： 清洗过 热门行业报告字段 序号 字段 类型 名称 备注 1 menu string 行业名称 中文 2 abstract string 报告摘要 3 title string 标题 4 paper_url string 文件下载url 5 date string 时间 6 paper_from string 文件来源 7 paper string 文件路径 8 author string 文件作者 多个以逗号隔开 9 update_time string 更新时间 时间戳形式 10 parent_id string 父级id 11 sign string 签名 12 cleaning_status int 清洗位 默认为0 "},"part1_jskj/basicUrl.html":{"url":"part1_jskj/basicUrl.html","title":"基本网站汇总","keywords":"","body":"基本网站汇总 目录索引 1. 常用数据公开网站 2. 政府开放数据 3. 数据竞赛网站 4. 财经数据 5. 网贷数据 6. 公司年报 7. 指数查询 8. 金融财经数据 9. 报告分析 10. 地理数据：水土气候数据 11. 环境数据 12. 遥感数据 13. 其它自然人文数据 14. 影像数据 15. 其它国家数据 16. 其它细分行业数据 17. 附加 具体内容 1、常用数据公开网站 序号 网站名 网址 备注 1 前瞻网 https://d.qianzhan.com/ 2 国家数据 https://data.stats.gov.cn/ 3 艾媒网 https://www.iimedia.cn/ 4 觅途云 http://5gb1hc.wozblog.com/data_/ 5 UCI http://archive.ics.uci.edu/ml/index.php 6 国家统计局 https://data.stats.gov.cn/ 7 CEIC https://www.ceicdata.com/zh-hans 8 万得 https://www.wind.com.cn/ 9 搜数 http://www.soshoo.com/index.do# 10 中国统计信息网 http://www.cnstats.org/ 11 亚马逊aws https://registry.opendata.aws/ 12 FIGSHARE https://figshare.com/ 13 IT桔子 https://www.itjuzi.com/ 2、政府开放数据 序号 网站名 网址 备注 1 北京市政务数据资源网 https://data.beijing.gov.cn/ 2 深圳市政府数据开放平台 https://opendata.sz.gov.cn/ 3 上海市政务数据服务网 https://data.sh.gov.cn/ 4 贵州省政府数据开放平台 http://data.guizhou.gov.cn/#!/openData 3、数据竞赛网站 序号 网站名 网址 备注 1 DataCastle https://www.dcjingsai.com/v2/index.html 2 kaggle https://www.kaggle.com/ 3 天池 https://tianchi.aliyun.com/dataset 4 Datafountain https://www.datafountain.cn/ 4、财经数据 序号 网站名 网址 备注 1 新浪财经 https://finance.sina.com.cn/ 2 东方财富网 https://www.eastmoney.com/ 3 中财网 http://cfi.net.cn/ 4 黄金头条 https://www.goldtoutiao.com/home 5 StockQ http://www.stockq.org/ 6 Investing https://cn.investing.com/ 5、网贷数据 序号 网站名 网址 备注 1 网贷之家 https://shuju.wdzj.com/ 2 零壹数据 http://data.01caijing.com/ 3 网贷天眼 https://www.p2peye.com/ 6、公司年报 序号 网站名 网址 备注 1 巨潮资讯网 http://www.cninfo.com.cn/new/index 2 HKEx news披露易 https://www.hkexnews.hk/index_c.htm 3 清博大数据 http://www.gsdata.cn/ 4 优易数据 https://yqfw.cdyoue.com/ 5 数据堂 https://www.datatang.com/ 7、指数查询 序号 网站名 网址 备注 1 百度指数 https://index.baidu.com/v2/index.html#/ 2 阿里指数 https://dt.alibaba.com/alizs.htm 3 友盟+ https://compass.umeng.com/ 4 爱奇艺指数 http://index.iqiyi.com/ 5 头条指数 https://index.toutiao.com/ 6 好搜指数 https://trends.so.com/ 7 搜狗指数 http://zhishu.sogou.com/ 8、金融财经数据 序号 网站名 网址 备注 1 同花顺数据中心 http://data.10jqka.com.cn/ 2 和讯数据 http://data.hexun.com/ 3 零壹财经 https://www.01caijing.com/data/index.htm 4 金融数据网 http://dc.xinhua08.com/ 5 萝卜投研 https://robo.datayes.com/v2/home 6 金融界 http://www.jrj.com.cn/ 7 东方财富网 https://www.eastmoney.com/ 8 吉林金融网 http://www.jl.chinafinance.net.cn/ 9 搜狐证券 https://q.stock.sohu.com/zs/000001/index.shtml 10 CCER经济金融数据 https://www.wisers.com.cn/finance 11 香港金融管理局 https://www.hkma.gov.hk/chi/ 12 世界未来 http://www.wefore.com/show-model3-7.html 13 新浪财经 http://finance.sina.com.cn/mac/ 14 CEIC https://www.ceicdata.com/zh-hans 15 INSEE数据 https://www.insee.fr/en/accueil 16 投中研究院 https://www.chinaventure.com.cn/report/list.html 9、报告分析 序号 网站名 网址 备注 1 易观智库 https://www.analysys.cn/ 2 艾瑞网 https://www.iresearch.cn/ 3 艾媒网 https://www.iimedia.cn/ 4 CBNDATA https://www.cbndata.com/report 5 阿里研究院 http://www.aliresearch.com/cn/index 6 360研究报告 https://zt.360.cn/report/ 7 中国互联网信息中心 http://www.cnnic.net.cn/hlwfzyj/ 8 中国信通院 http://www.caict.ac.cn/kxyj/qwfb/bps/ 9 中国互联网数据平台 http://www.cnidp.cn/ 10 清博大数据 http://www.gsdata.cn/ 11 数据观 http://www.cbdio.com/ 12 腾讯大数据 https://data.qq.com/ 13 大数据世界 http://www.thebigdata.cn/ 10、地理数据：水土气候数据 序号 网站名 网址 备注 1 水土保持生态建设网 http://www.swcc.org.cn/ 2 黄河风情 http://www.yellowriver.org/ 3 黄河流域数据中心 http://henu.geodata.cn/ 4 国家地球系统科学数据中心 http://loess.geodata.cn/ 5 数字黑河 http://heihe.westgis.ac.cn/zh-hans/ 6 国家气象科学数据中心 http://data.cma.cn/ 7 NOAA https://www.noaa.gov/ 8 气候研究所 http://www.cru.uea.ac.uk/ 11、环境数据 序号 网站名 网址 备注 1 国家青藏高原科学数据中心 http://westdc.westgis.ac.cn/zh-hans/ 2 国家地球系统科学数据中心 http://loess.geodata.cn/ 12、遥感数据 1 甘肃省基础地理信息中心 http://www.cehuiju.gansu.gov.cn/ 2 对滴观测数据共享计划 http://ids.ceode.ac.cn/ 13、其它自然人文数据 序号 网站名 网址 备注 1 地理国情监测云平台 http://www.dsac.cn/ 2 中国资源卫星应用中心 http://www.cresda.com/CN/ 3 资源学科创新平台 http://www.data.ac.cn/ 4 中国科学院资源环境科学数据中心 http://www.resdc.cn/ 5 国家地球系统科学数据中心 http://www.geodata.cn/ 6 光谱数据分析网站 https://modis.gsfc.nasa.gov/ 14、影像数据 序号 网站名 网址 备注 1 NOAA CLASS https://www.avl.class.noaa.gov 15、其它国家数据 序号 网站名 网址 备注 1 新加坡国家数据 https://data.gov.sg/ 2 美国国家数据 https://www.data.gov/ 3 法国国家数据 https://www.data.gouv.fr/en/ 4 英国国家数据 https://data.gov.uk/ 5 国家数据 https://data.stats.gov.cn/ 6 中国统计年鉴 http://www.stats.gov.cn/tjsj/ndsj/ 7 年鉴汪 https://www.nianjianwang.com/ 16、其它细分行业数据 序号 网站名 网址 备注 1 中国报告大厅 http://www.chinabgao.com/stat 2 dataEye https://www.dataeye.com/report 3 CBO中国票房指数 https://www.endata.com.cn 4 易车指数 http://index.bitauto.com 5 房天下 https://fdc.fang.com/index/ 6 高德地图 https://report.amap.com 17、附加 序号 网站名 网址 备注 1 全球采购网 http://www.globalbuy.cc/ 2 中国金融信息网 http://www.xinhua08.com/ 3 中国互联网络信息中心 http://cnnic.net.cn/ 4 融汇岛 http://rhd361.com/ 5 产业检测 https://www.hsmap.com/ 6 极光数据 https://www.jiguang.cn/ 7 DCCI互联网数据中心 http://www.dcci.com.cn/ 8 中经研究 http://kexingxing.cn/ 9 北极星大气网 http://daqi.bjx.com.cn/ 10 中国产业规划网 http://chanyeguihua.com/ 11 企查猫 https://www.qichamao.com/ 12 普华永道 https://www.pwccn.com/zh.html 13 毕马威 https://home.kpmg/cn/zh/home.html 14 摩根士丹利 https://www.morganstanleychina.com/ "},"part1_jskj/hot_industry.html":{"url":"part1_jskj/hot_industry.html","title":"热门行业","keywords":"","body":"热门行业展示 热门行业子类 热门行业 负责人 行业分类 ######## ####### ###### 文化产业 李阿兵 图书出版业 新闻传媒产业 广播影视业 动漫产业 网络文化产业 表演艺术业 广告产业 休闲产业（体育赛事，休闲娱乐业，文化旅游业等）以及会展产业 信息技术 程厚富 信息处理和服务产业 信息处理设备行业 信息传递中介行业 信息安全技术产业 节能环保 郑宇飞 高效节能技术和装备产业 高效节能产品 节能服务产业 先进环保技术和装备 环保产品 环保服务产业 节能照明产业 资源循环利用产业 大气污染治理产业 生态环境产业（生态保护生态修复） 清洁生产产业 清洁能源产业 基础设施绿色升级产业 绿色服务产业 电气设备 陈志霖 锅炉及辅助设备制造业 汽轮机及辅机制造业 水轮机及辅机制造业 风能原动设备制造 发电机及发电机组制造业 电机 输配电设备 电线电缆 电池电源 电器设备产业 纺织服饰 吴健爱 纺织原料产业 服装鞋帽产业 皮革箱包产业 印染 珠宝首饰 户外用品 羽绒 服装业、家用纺织品、产业用纺织品 棉纺织、化纤、麻纺织、毛纺织、丝绸、纺织品针织产业、印染业 旅游酒店 邓俊 酒店配套设备产业；酒店服务员及管理人员培训；酒店管理软件系统；酒店营销策划 旅游出行 旅游住宿 旅游餐饮 旅游游览 旅游娱乐 钢铁有色 黄伟皓 采矿业 钢铁 有色金属 黑色金属 金属冶炼 金属加工及制品 冶金产业 复合材料 新材料产业 医疗健康 吴邦福 医疗救治产业 药械交易产业 优生优育产业 健康养老产业 康复医疗产业 公共医疗卫生产业 保健用品产业 医药制造产业 医疗器械产业 传统中药产业 民族医药产业 生命健康管理产业 子类别 分类类别#### 子类别1 子类别2 子类别3 子类别4 子类别5 子类别6 子类别7 新闻资讯 中央精神 部委政策 时政新闻 产经动态 今日焦点 专家访谈 政策解读 产业图谱 产业细分 行业细分 产业园区 前后产业链 产业区块链 产业生态圈 市场分布图 指数数据 产业发展指数 招商大数据 项目大数据 专利大数据 价格大数据 行业排行榜 关键数据发布 企业运营 名企排行榜 品牌排行榜 企业业务布局 上市企业分析 企业信用评价 企业营商环境 企业状态查询 产融结合 产业投融资信息 企业上市融资 项目融资租赁 产业基金信托 公私合作投融资 企业融资并购 总投资与结构 科创平台 科技创新规划 政府科创平台 园区服务体系 孵化项目信息 优秀研发团队 科技成果转化 创新战略联盟 产业监测 政府主管部门 民间行业协会 行业标准规范 行业数据监测 产业舆情监测 产业危机预警 产业风险防控 职教技培 委托招录培训 资格认证培训 技能岗位培训 产教融合模式 校企合作模式 行业技能大赛 产业培训联盟 研究报告 可行性报告 商业计划书 规划设计方案 专项调研报告 科技成果评价 产业技术咨询 产经评论分析 展会论坛 高端论坛 国际论坛 主题峰会 技术博览会 产品交易会 专业技术交流会 云端线上展览 "},"part2_docker/":{"url":"part2_docker/","title":"二、docker服务","keywords":"","body":"Docker资源 Docker官方英文资源： docker官网：http://www.docker.com Docker windows入门：https://docs.docker.com/windows/ Docker Linux 入门：https://docs.docker.com/linux/ Docker mac 入门：https://docs.docker.com/mac/ Docker 用户指引：https://docs.docker.com/engine/userguide/ Docker 官方博客：http://blog.docker.com/ Docker Hub: https://hub.docker.com/ Docker开源： https://www.docker.com/open-source Docker中文资源： Docker中文网站：http://www.docker.org.cn Docker入门教程: http://www.docker.org.cn/book/docker.html Docker安装手册：http://www.docker.org.cn/book/install.html 一小时Docker教程 ：https://blog.csphere.cn/archives/22 Docker纸质书：http://www.docker.org.cn/dockershuji.html DockerPPT：http://www.docker.org.cn/dockerppt.html "},"part2_docker/scrapyd&web.html":{"url":"part2_docker/scrapyd&web.html","title":"scrapyd&scrapydweb","keywords":"","body":"1、scrapyd镜像 系统介绍 基础镜像:python:3.7-buster 系统: ubuntu系统 python环境: py3.7.8 pip版本: pip3 版本为 20.2.2 系统apt源: 阿里源 pip源: 豆瓣源:https://pypi.douban.com/simple/ 容器中 pip-->pip3 pip3-->pip3 python-->python3.7 python3-->python3.7 安装pip库: requirement.txt + pycommon0.0.6 + logparser 安装编辑软件: vim 环境变量: PYTHONIOENCODING=utf-8, TZ=Asia/Shanghai 映射端口: 6800 1、准备镜像 1.1 基础镜像 # 拉取镜像 sudo docker pull python:3.7-buster # 生成镜像 sudo docker build -t py37/scrapyd . 1.2 编写Dockerfile文件 FROM python:3.7-buster # 换源,并更新源 RUN rm /etc/apt/sources.list COPY sources.list /etc/apt/sources.list RUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 40976EAF437D05B5 RUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 RUN apt-get update && apt-get -y upgrade RUN apt-get install -y vim # 设置工作目录 RUN mkdir /app # 选择工作文件夹 WORKDIR /app # 设置环境变量 ENV PYTHONIOENCODING=utf-8 ENV TZ=Asia/Shanghai # 安装包 COPY requirement.txt /app RUN pip install -r requirement.txt -i https://pypi.douban.com/simple/ EXPOSE 6800 CMD [\"scrapyd\"] 1.3 sources.list文件 使用阿里源 deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 1.4 预安装py库 requirement.txt scrapyd==1.2.1 scrapyd-client==1.1.0 requests==2.22.0 pymongo==3.9.0 PyMySQL==0.9.3 Scrapy==1.8.0 fake-useragent==0.1.11 lxml==4.4.2 selenium==3.141.0 retrying==1.3.3 PyExecJS==1.5.1 bs4==0.0.1 uuid==1.30 peewee==3.13.3 retrying==1.3.3 1.5 运行指令 (要求:docker命名和挂载目录名称都是 --> scrapyd你的名字你的端口, 例如scrapyd_lab_6822,可以在一下命令后面指定运行指令:/app/start.sh,默认为scrapyd) sudo docker run -it -d --name [scrapyd_xxx_68xx] --restart=always -p 68xx:6800 -v /scrapyd_apps/scrapyd_xxx_6800:/app py37/scrapyd 2、scrapydweb镜像 基础镜像：python:3.7-buster Dockerfile文件 FROM python:3.7-buster # 换源 RUN rm /etc/apt/sources.list COPY sources.list /etc/apt/sources.list RUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 40976EAF437D05B5 RUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 RUN apt-get update && apt-get -y upgrade RUN apt-get install -y vim # 设置工作目录 RUN mkdir /app WORKDIR /app # 设置环境变量 ENV PYTHONIOENCODING=utf-8 ENV TZ=Asia/Shanghai # 安装包 RUN pip install scrapydweb -i https://pypi.douban.com/simple/ EXPOSE 5000 CMD [\"scrapydweb\"] 运行指令 # 生成镜像 sudo docker build -t py37/scrapydweb . # 生成容器 sudo docker run -it -d --name scrapydweb_5000 --restart=always -p 5000:5000 -v /scrapyd_apps/scrapydweb:/app py37/scrapydweb "},"part2_docker/mongo_cluster.html":{"url":"part2_docker/mongo_cluster.html","title":"mongo集群","keywords":"","body":"mongo集群分片 概念文章1：https://developer.ibm.com/zh/articles/os-mongodb-sharded-cluster/ 操作文章2：https://blog.csdn.net/yekoufeng/article/details/83412431#mongosmongos_103 操作视频：https://www.bilibili.com/video/BV1p4411J7sq?from=search&seid=3909948528816913046 配置文件1: https://www.jb51.net/article/161315.htm https://www.cnblogs.com/phpandmysql/p/7763394.html 搭建集群(docker伪集群) 集群角色 docker名称 docker内部端口 挂载路径 介绍 Config Server configsvr0 10.1.1.2:27019 /home/mongodb/data/cs/configsvr0 服务复制集 Config Server configsvr1 10.1.1.3:27019 /home/mongodb/data/cs/configsvr1 服务复制集 Shard Server shardsvr00 10.1.1.4:27018 /home/mongodb/data/sh/shardsvr00 分片复制集(数据存储) Shard Server shardsvr01 10.1.1.5:27018 /home/mongodb/data/sh/shardsvr01 分片复制集 Shard Server shardsvr10 10.1.1.6:27018 /home/mongodb/data/sh/shardsvr10 分片复制集 Shard Server shardsvr11 10.1.1.7:27018 /home/mongodb/data/sh/shardsvr11 分片复制集 Mongos mongos0 10.1.1.8:27017 无 连接(路由) Mongos mongos1 10.1.1.9:27017 无 备用连接 1、拉取mongo镜像 # 拉取mongo镜像 docker pull docker # 为MongoDB集群创建独立的docker网桥 docker network create --subnet=10.1.1.0/24 mongodb0 2、创建配置复制集 运行三个配置复制集 # 配置复制集1 docker run -d --name configsvr0 --network=mongodb0 --ip=10.1.1.2 --restart=always -v /home/mongodb/data/cs/configsvr0:/data/configdb mongo --configsvr --replSet \"rs_configsvr\" --bind_ip_all --wiredTigerCacheSizeGB 4 # 配置复制集2 docker run -d --name configsvr1 --network=mongodb0 --ip=10.1.1.3 --restart=always -v /home/mongodb/data/cs/configsvr1:/data/configdb mongo --configsvr --replSet \"rs_configsvr\" --bind_ip_all --wiredTigerCacheSizeGB 4 # 配置复制集3 docker run -d --name configsvr2 --network=mongodb0 --ip=10.1.1.4 --restart=always -v /home/mongodb/data/cs/configsvr2:/data/configdb mongo --configsvr --replSet \"rs_configsvr\" --bind_ip_all --wiredTigerCacheSizeGB 4 # 另外，可以指定配置文件 默认无配置文件 docker run --name some-mongo -v /my/custom:/etc/mongo -d mongo --config /etc/mongo/mongod.conf 查询复制集的ip地址 docker inspect configsvr0 | grep IPAddress docker inspect configsvr1 | grep IPAddress docker inspect configsvr2 | grep IPAddress 由于–configsvr的默认端口为27019，所以我的配置服务的地址为 configsvr0: 10.1.1.2:27019 configsvr1: 10.1.1.3:27019 configsvr2: 10.1.1.4:27019 初始化配置复制集 # 进入容器 docker exec -it configsvr0 bash # 进入mongo mongo --host 10.1.1.2 --port 27019 # mongo中运行 rs.initiate({ _id: \"rs_configsvr\", configsvr: true, members: [ { _id : 0, host : \"10.1.1.2:27019\" }, { _id : 1, host : \"10.1.1.3:27019\" }, { _id : 2, host : \"10.1.1.4:27019\" } ] }) 3、创建分片复制集 # 第一个分片复制集 docker run --name shardsvr00 --network=mongodb0 --ip=10.1.1.5 --restart=always -d -v /home/mongodb/data/sh/shardsvr00:/data/db mongo --shardsvr --replSet \"rs_shardsvr0\" --bind_ip_all docker run --name shardsvr01 --network=mongodb0 --ip=10.1.1.6 --restart=always -d -v /home/mongodb/data/sh/shardsvr01:/data/db mongo --shardsvr --replSet \"rs_shardsvr0\" --bind_ip_all docker run --name shardsvr02 --network=mongodb0 --ip=10.1.1.7 --restart=always -d -v /home/mongodb/data/sh/shardsvr02:/data/db mongo --shardsvr --replSet \"rs_shardsvr0\" --bind_ip_all # 第二个分片复制集 docker run --name shardsvr10 --network=mongodb0 --ip=10.1.1.8 --restart=always -d -v /home/mongodb/data/sh/shardsvr10:/data/db mongo --shardsvr --replSet \"rs_shardsvr1\" --bind_ip_all docker run --name shardsvr11 --network=mongodb0 --ip=10.1.1.9 --restart=always -d -v /home/mongodb/data/sh/shardsvr11:/data/db mongo --shardsvr --replSet \"rs_shardsvr1\" --bind_ip_all docker run --name shardsvr12 --network=mongodb0 --ip=10.1.1.10 --restart=always -d -v /home/mongodb/data/sh/shardsvr12:/data/db mongo --shardsvr --replSet \"rs_shardsvr1\" --bind_ip_all # 第三个分片复制集 docker run --name shardsvr20 --network=mongodb0 --ip=10.1.1.11 --restart=always -d -v /home/mongodb/data/sh/shardsvr20:/data/db mongo --shardsvr --replSet \"rs_shardsvr2\" --bind_ip_all docker run --name shardsvr21 --network=mongodb0 --ip=10.1.1.12 --restart=always -d -v /home/mongodb/data/sh/shardsvr21:/data/db mongo --shardsvr --replSet \"rs_shardsvr2\" --bind_ip_all docker run --name shardsvr22 --network=mongodb0 --ip=10.1.1.13 --restart=always -d -v /home/mongodb/data/sh/shardsvr22:/data/db mongo --shardsvr --replSet \"rs_shardsvr2\" --bind_ip_all 查询复制集的ip地址 docker inspect shardsvr00 | grep IPAddress docker inspect shardsvr01 | grep IPAddress docker inspect shardsvr02 | grep IPAddress docker inspect shardsvr10 | grep IPAddress docker inspect shardsvr11 | grep IPAddress docker inspect shardsvr12 | grep IPAddress docker inspect shardsvr20 | grep IPAddress docker inspect shardsvr21 | grep IPAddress docker inspect shardsvr22 | grep IPAddress 由于–configsvr的默认端口为27018，所以我的配置服务的地址为 shardsvr00: 10.1.1.5:27018 shardsvr01: 10.1.1.6:27018 shardsvr02: 10.1.1.7:27018 shardsvr10: 10.1.1.8:27018 shardsvr11: 10.1.1.9:27018 shardsvr12: 10.1.1.10:27018 shardsvr20: 10.1.1.11:27018 shardsvr21: 10.1.1.12:27018 shardsvr22: 10.1.1.13:27018 初始化配置复制集 # 进入容器 docker exec -it shardsvr00 bash # 进入mongo mongo --host 10.1.1.5 --port 27018 # mongo中运行 rs.initiate({ _id : \"rs_shardsvr0\", members: [ { _id : 0, host : \"10.1.1.5:27018\" }, { _id : 1, host : \"10.1.1.6:27018\" }, { _id : 2, host : \"10.1.1.7:27018\" } ] }) # 进入mongo mongo --host 10.1.1.8 --port 27018 # mongo中运行 rs.initiate({ _id : \"rs_shardsvr1\", members: [ { _id : 0, host : \"10.1.1.8:27018\" }, { _id : 1, host : \"10.1.1.9:27018\" }, { _id : 2, host : \"10.1.1.10:27018\" } ] }) # 进入mongo mongo --host 10.1.1.11 --port 27018 # mongo中运行 rs.initiate({ _id : \"rs_shardsvr2\", members: [ { _id : 0, host : \"10.1.1.11:27018\" }, { _id : 1, host : \"10.1.1.12:27018\" }, { _id : 2, host : \"10.1.1.13:27018\" } ] }) 4、创建mongos，连接mongos到分片集群 由于镜像的默认入口是 mongod，所以要通过 --entrypoint “mongos” 将其改为 mongos： docker run --name mongos0 --network=mongodb0 --ip=10.1.1.14 --restart=always -d -p 27017:27017 --entrypoint \"mongos\" mongo --configdb rs_configsvr/10.1.1.2:27019,10.1.1.3:27019,10.1.1.4:27019 --bind_ip_all # 第二个 docker run --name mongos1 --network=mongodb0 --ip=10.1.1.15 --restart=always -d -p 27018:27017 --entrypoint \"mongos\" mongo --configdb rs_configsvr/10.1.1.2:27019,10.1.1.3:27019,10.1.1.4:27019 --bind_ip_all 查看地址 docker inspect mongos0 | grep IPAddress docker inspect mongos1 | grep IPAddress 默认端口为27017，故地址为： 10.1.1.14：27017 10.1.1.14：27017 注：映射到宿主机便于外面客户端访问 5、添加分片到集群 docker exec -it mongos0 bash mongo --host 10.1.1.14 --port 27017 # mongo中运行 sh.addShard(\"rs_shardsvr0/10.1.1.5:27018,10.1.1.6:27018,10.1.1.7:27018\") sh.addShard(\"rs_shardsvr1/10.1.1.8:27018,10.1.1.9:27018,10.1.1.10:27018\") sh.addShard(\"rs_shardsvr2/10.1.1.11:27018,10.1.1.12:27018,10.1.1.13:27018\") 其它操作 # mongos中 # 查看分片 db.runCommand({listshards:1}) # 查看分片状态 sh.status() 6、数据库启用分片 # 上一步不退出docker 继续运行 sh.enableSharding(\"industry\") sh.enableSharding(\"national_conditions\") 7、集合的分片操作 7.1 基于Ranged的分片操作 基于范围分片特别适合范围查找，因为可以直接定位到分片，所以效率很高。 分片的键必须是索引(也可以为联合索引) 以下为具体操作步骤： 开启 test 库的分片功能。 sh.enableSharding(\"test\") 选择集合的分片键，此时 MongoDB 会自动为 age 字段创建索引。 sh.shardCollection(\"test.test_shard\",{\"age\": 1}) sh.shardCollection(\"industry.C25_data\",{\"_id\":1}) 批量造测试数据。 use test for (i = 1; i 观察分片效果。以下为命令和部分输出示例： sh.status() test.test_shard shard key: { \"age\" : 1 } unique: false balancing: true chunks: rep_shard1 2 rep_shard2 3 { \"age\" : { \"$minKey\" : 1 } } --从输出结果可以看到 test.test_shard 集合总共有 2 个分片，分片 rep_shard1 上有 2 个 chunk，分片 rep_shard2 上有 3 个 chunk，年龄大于或等于 0 并且小于 36 的文档数据放到了第一个分片 rep_shard1，年龄大于或等于 36 并且小于 73 的文档数据放到了第二个分片 rep_shard2，此时已经达到了分片的效果。我们可以使用 find 命令来确认是否对应的数据存在相应的分片，以下为命令和部分输出示例： db.test_shard.find({ age: { $gte : 36 ,$lt : 73 } }).explain() { \"queryPlanner\" : { \"winningPlan\" : { \"stage\" : \"SINGLE_SHARD\", \"shards\" : [ { \"shardName\" : \"rep_shard2\", \"connectionString\" : \"rep_shard2/10.0.4.6:27019,10.0.4.7:27019,10.0.4.8:27019\", \"namespace\" : \"test.test_shard\", \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"SHARDING_FILTER\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"age\" : 1 }, \"indexName\" : \"age_1\", \"direction\" : \"forward\", \"indexBounds\" : { \"age\" : [ \"[36.0, 73.0)\" ] } } } }, } ] } } } 从以上输出结果可以看到，当查找年龄范围为大于等于 36 并且小于 73 的文档数据，MongoDB 会直接定位到分片 rep_shard2，从而避免全分片扫描以提高查找效率。如果将 $gte : 36 改为 $gte : 35 ，结果会是怎么样的呢？答案是 MongoDB 会扫描全部分片，执行计划的结果将由 SINGLE_SHARD 变为 SHARD_MERGE ，如果感兴趣，您可以自行验证。 7.2 基于Hashed的分片操作 开启 test 库的分片功能。 sh.enableSharding(\"test\") 显示更多 选择集合的分片键，注意这里创建的是 hash 索引。 sh.shardCollection(\"test.test_shard\",{\"age\": \"hashed\"}) 显示更多 批量造测试数据。 use test for (i = 1; i 显示更多 观察分片效果。以下为命令和部分输出示例： sh.status() chunks: rep_shard1 2 rep_shard2 2 { \"age\" : { \"$minKey\" : 1 } } --通过以上输出可以看到，对于等值查找，基于 Hashed 分片查找效率很高，直接定位到一个分片就可以返回满足条件的数据，无需进行全部分片的查找。 其它命令 删除命令 db.runCommand({removeShard:\"rs_shardsvr1\"}) balaece 配置修改 详情请看mongo中文社区 # 找到主节点 cfg = rs.conf(); cfg.members[0].host = \"10.1.1.5:27019\" cfg.members[1].host = \"10.1.1.6:27019\" cfg.members[2].host = \"10.1.1.7:27019\" rs.reconfig(cfg) "},"part2_docker/other_docker.html":{"url":"part2_docker/other_docker.html","title":"其它服务","keywords":"","body":"mysql docker run -p 3306:3306 --name mysql \\ -v /home/mysql/conf:/etc/mysql \\ -v /home/mysql/logs:/var/log/mysql \\ -v /home/mysql/data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=jskj61776099 \\ -d mysql:5.7 redis 从官网获取 redis.conf 配置文件 修改默认配置文件 bind 127.0.0.1 #注释掉这部分，这是限制redis只能本地访问 protected-mode no #默认yes，开启保护模式，限制为本地访问 daemonize no#默认no，改为yes意为以守护进程方式启动，可后台运行，除非kill进程（可选），改为yes会使配置文件方式启动redis失败 dir ./ #输入本地redis数据库存放文件夹（可选） appendonly yes #redis持久化（可选） docker 启动 redis 命令 docker run -p 6379:6379 --restart=always --name redis --privileged=true -v /home/redis/redis.conf:/etc/redis/redis.conf -v /home/redis/data:/data -d redis redis-server /etc/redis/redis.conf proxy_pool docker run --env DB_CONN=redis://192.168.3.85:6379/0 -p 5010:5010 -v /home/proxypool:/app --net=host --name proxypool -d jhao104/proxy_pool:latest gitbook docker create -v /home/book:/srv/gitbook -p 4000:4000 --name gbook fellah/gitbook pythonFile Dockerfile文件 docker create -it -v /home/pythonFile:/app --name pythonFile python37:lab python /app/run.py "},"part3_tech/":{"url":"part3_tech/","title":"三、技术操作","keywords":"","body":"爬虫环境 环境 anaconda： 见以下国内知名镜像源 docker： 见以下国内知名镜像源 mongo server官网下载：https://www.mongodb.com/try/download/community mysql Installer官网下载：https://dev.mysql.com/downloads/mysql/ redis 官网下载：https://redis.io/download git下载：https://github.com/waylau/git-for-win/ nodejs下载 : http://nodejs.cn/download/ 常用pip源 阿里云 http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 豆瓣(douban) http://pypi.douban.com/simple/ 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ 中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/ 国内知名镜像源 清华大学开源软件镜像站 北京外国语大学开源软件镜像站 上海交通大学开源软件镜像站 中国科技大学开源镜像站 "},"part3_tech/proj_rule.html":{"url":"part3_tech/proj_rule.html","title":"项目规范","keywords":"","body":"Scrapy项目编写及布署流程规范 一、编写规范 1.项目名 名字使用驼峰（CameCase）命名风格，首字母大写。 框架名网站名_V[数字编号]行业编号 Scrapy_ChinaNet_V1_01 当网站名过长，可使用网站名首字母起名 如Scrapy_ChinaLianTongTiDong_V1_01 --》 Scrapy_CLTTD_V1_01 注意：所有项目的名称不能重复 2.爬虫名 名字使用驼峰（CameCase）命名风格，首字母小写。 网站名_v[数字编号] chinaNet_v1 3.类名 #类名使用驼峰（CameCase）命名风格，首字母大写，私有类可以用一个下划线开头。 class Farm(): pass class AnimalFarm(Farm): pass class _PrivateFarm(Farm): pass 4.函数名 #函数名一律小写，多个单词，用下划线隔开 def run(): pass def run_with_env(): Pass #私有函数在函数前加一个下划线_ class Person(): def _private_func(): pass 5.变量名 变量名用小写，多个单词，用下划线_隔开 if __name__ == '__main__': count = 0 school_name = '' 6.常量名 采用全大写，多个单词，使用下划线隔开 MAX_CLIENT = 100 MAX_CONNECTION = 1000 CONNECTION_TIMEOUT = 600 7.注释 对类添加说明和重要的函数或代码段添加注释 单行注释用# 多行注释用三引号''' ''' 8. Scrapy项目文件要求 Scrapy_ProjectName_V1_01 └─Scrapy_ProjectName_V1_01 │ ├─spiders │ │ └─spider.py │ └─items.py │ └─middlewares.py │ └─pipelines.py │ └─settings.py └─scrapy.cfg 8.1 spider.py 爬虫文件 代码规范、整洁 在必要地方加上注释 部署前必须要加上去重功能 注意：不要在此做数据入库操作，要推送到pipelines 8.2 Settings.py爬虫配置文件 必须使用 下载延迟：DOWNLOAD_DELAY = 3 （3秒以上） 下载中间件：DOWNLOADER_MIDDLEWARES 数据管道：ITEM_PIPELINES 日志等级：LOG_LEVEL = 'INFO' Apollo配置信息 其他功能视情况自由使用 8.3 pipelines.py： spider爬取的数据一定要推送到pipelines中，这样可以通过爬虫日志观察到数据统计情况 主要方法： open_spider(self,spider):在此方法中定义参数变量，数据库初始化 process_item(self,item,spider):在此方法中进行数据的清洗、去重、入库，在入库前要判断字段是否空缺，类型是否正确，合格才入库 close_spider(self,spider):在此方法中关闭数据库连接，避免数据库占用内存 执行顺序：open_spider --> process_item --> close_spider 注意：涉及数据库的信息如地址、密码、端口，以及一切调用接口链接如获取代理链接、selenium_webdriver接口链接等都需要使用apollo，在apollo中获取。 8.4 middlewares.py 在此编写下载中间件功能 添加代理、更换代理 添加用户代理 添加超时处理 处理请求异常 Selenium，webdriver 主要方法及执行顺序：process_request --> process_response --> process_exception 注意：涉及数据库的信息如 地址、密码、端口，以及一切调用接口链接如获取代理链接、selenium_webdriver接口链接等都需要使用apollo，在apollo中获取。 8.5 items.py 添加爬取数据字段 8.6 scrapy.cfg 添加布署地址信息 [scrapyd:demo] url = http://xxxxxx:xxxx project = 项目名 8.7 其他 除了以上说明的文件，还可以自行添加文件加以辅助 9. scrapy项目的一生 二、 布署 1. scrapyd： 环境安装配置 2. scrapydweb： 定时要求： 定时的爬取时间不能集中在某个时间段； 资讯类爬虫爬取频率是每天爬取一次或者每周爬取几次，视情况而定； 数据类、报告类爬虫频率根据数据类型及网站类型而定，一般为1个月一次，或两三个月一次，日更的网站可以每日爬取一次； 3. Git推送： 创建一个gitlab的仓库项目，复制http的git地址 git clone [url.git] 下载到本地 进入到刚下载的git 文件夹， 把你要上传的文件，复制到此文件夹，修改README.md文件 git add . ----> git commit -m\"your commit message\"----> git push Jenkins布署： 查看jenkins说明：地址 三、其他 1. 常用工具 1.1. 代理池 http://192.168.3.85:5010/get/ 获取代理 http://192.168.3.85:5010/get_status/ 查看代理数量 1.2. Apollo（阿波罗） 详情请看Apollo说明 1.3. Selenium http://192.168.3.85:4444/wd/hub/ 1.4. scrapyd-scrapydweb: http://192.168.3.85:5000/1/servers/ 2. 数据、报告菜单id生成说明 如 农业 id 为01 目录 名称 id 第一级目录 农业 01 第二级目录 xxx 01001 第三级目录 xxx 01001001 ...... ...... ...... 最后一级目录 xxx 01001…001 Mysql菜单表字段必须有这3个字段 menu_id string menu_name string parent_menu_id string 其他字段按个人情况添加 "},"part3_tech/Apollo.html":{"url":"part3_tech/Apollo.html","title":"Apollo配置说明","keywords":"","body":"Apollo 动态配置 介绍 Apollo（阿波罗）来自于携程研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。 1、安装 下载：安装此安装包 安装：pip install Scrapy-2.3.0.0.tar.gz 这个包是scrapy的魔改版本，添加了对apollo的支持。Scrapy中所有的settings配置都可添加到apollo中。 2、Apollo使用步骤 进入apollo管理页面： Apollo网址：http://192.168.3.85:8070/ 登录账号：名字拼音 登录密码: 123456 打开后Apollo管理页面： 点击新建项目 部门可以选择第一个就好，AppID是当前配置的唯一id，不要重复。应用名称 是当前的配置的名字，最好有标志性。项目负责人选择自己。 创建完成之后，进入主页面： 点击新增配置，把settings中的配置以键值对的形式存进去，然后保存。保存完毕点击那个绿色的发布按钮进行配置发布。 Apollo的配置文件创建就这样。 3、在scrapy中的应用 进入爬虫项目的settings.py文件， 添加如下： # apollo 配置中心 APP_ID = 'scrapy_config_common,Scrapy_config_quota, Scrapy_config_GUOYAN' CLUSTER = 'default' CONFIG_SERVER_URL = 'http://192.168.3.85:8096/' 这个APP_ID就是之前创建的apollo配置中的appid， 如果该爬虫项目需要使用多个apollo配置文件，那么就把配置的appid之间用逗号分隔开。 注意：建议每个爬虫三个appid，第一个appid当做数据(资讯或报告)公共的common配置，所有爬虫都添加一个common的配置，common配置中可以存放mongo或者mysql数据库地址等信息；第二个appid设置成行业的公用配置；第三个appid设置成当前爬虫私有的配置，可以存放当前爬虫的数据表名等信息。方便调试。爬虫的appid不一定要在apollo中存在，所以即使APP_ID选项写了某个appid，但是apollo中没有这个appid配置，也不会出什么问题。 APP_ID = 'scrapy_config_common,Scrapy_config_quota, Scrapy_config_GUOYAN'。 越在后的appid的配置优先级越高，如果这三个appid的配置中具有某个相同key。那么以最后一个为准。 例如：scrapy_config_common,Scrapy_config_quota, Scrapy_config_GUOYAN这三个apollo配置中都有一个MONGO_HOST的key，那么将会以最后一个appid，即Scrapy_config_GUOYAN中的配置为准。 代码中使用apollo的配置： # py中获取Apollo配置 import pymogno from pybase.apollo_setting import get_project_settings class MongoPipeline(): config = get_project_settings def __init__(self): self.client = pymongo.MongoClient(host=self.config.get('MONGO_HOST'),...) 或者使用 from scrapy.utils.project import get_project_settings config = get_project_settings() config.get('MONGO_HOST') 获取配置就好 "},"part3_tech/pybase.html":{"url":"part3_tech/pybase.html","title":"pybase工具包","keywords":"","body":"pybase 1、介绍与安装 Pybase提供了一些公用方法，是一个工具包。 下载：点击这里 安装：pip install pycommon-pro-0.0.9.tar.gz pybase包含一些工具类 生成parent_id 集成了代理添加功能 添加公用管道 文件上传接口 2、用法介绍 2.1 生成parent_id工具类 Settings添加配置，可以设置在Apollo中 # Settings添加配置 G_ROOT_ID = 行业的分配id G_ROOT_NAME = 行业名称 G_TABLE_NAME = 菜单的数据表的名称 MYSQL_HOST = 192.168.0.11 MYSQL_PORT = 3306 MYSQL_DATABASE = '' MYSQL_USER = '' MYSQL_PASSWORD = '' Pybase模块中新增了一个自动生成工具类的工具，使用以下代码使用 from pybase.MenuIdGenerator import MenuIdGenerator class XXX(): # 初始化 def __init__(self): self.generator = MenuIdGenerator() # 使用 def xxl(self): Item[‘parent_menu_id’] = self.generator.getMenuId(['制造业', '酒、饮料和精制茶制造业', 'xxxx']) getMenuId方法返回菜单最后一级id，设置成当前数据的parent_menu_id就行了。。。。 2.2 添加代理功能 Pybase 模块中添加了公用的代理功能： Settings中添加配置： # 设置代理网址 PROXY_GET_URL='http://192.168.3.85:5010/get/' # 开启代理中间件 DOWNLOADER_MIDDLEWARES = { 'pybase.middlewares.AddProxyMiddlewares': 543, 'pybase.middlewares.MyRetryMiddleware': 534, } 添加完成即可请求时自动添加代理功能。当然了这两个配置都可以放在apollo中。 2.3 添加公用管道 Pybase模块中添加了公用管道功能： 用法如下 # Settings中添加配置： MONGO_HOST = '192.168.0.22' MONGO_PORT = '27017' MONGO_DATABASE = 'industry' MONGO_TABLE = 'C16_data' MONGO_USER = '' MONGO_PASSWORD = '' 如果是数据管道(根据parent_id, data_value, frequency, create_time去重)，还需要在settings中开启： ITEM_PIPELINES = { 'pybase.pipelines.ScrapyDataPipeline': 300, } 如果是资讯管道(根据news_id去重)，需要开启： ITEM_PIPELINES = { 'pybase.pipelines.ScrapyInfoPipeline': 300, } 如果是报告管道(根据content_id去重)： ITEM_PIPELINES = { 'pybase.pipelines.ScrapyReportPipeline': 300, } 2.4 文件上传功能 Pybase中添加了公用文件上传功能： 用法如下 from pybase.util import send_file send_file(\"ppp.jpg\", 'https://img.chihuogu.com/file/mstj/20200807/c7.jpg', \"http://192.168.3.85:8500/file/upload/spidername\") send_file参数解释： :param file_name：文件的名称 :param file_url：文件的url :param upload_url: 上传文件的接口(服务器上) :param headers: 图片下载的headers Send_file方法添加失败重试功能，重试三次 返回值： { \"code\": 1, \"msg\": \"success\", \"success\": True, \"data\": { \"fileName\": \"test.pdf\", \"url\": \"group1/M00/9A/DD/wKgAC19gcoSAfacSAACWxwCoGEw551.pdf\", \"filePath\": \"group1/M00/9A/DD/wKgAC19gcoSAfacSAACWxwCoGEw551.pdf\", \"fileSize\": 38599, \"flag\": True } } Code为1代表成功，其他全部为异常，data中url为返回的url，fileSize为文件大小 "},"part3_tech/Jenkins.html":{"url":"part3_tech/Jenkins.html","title":"jenkins","keywords":"","body":"Jenkins 1、介绍 jenkins是什么 Jenkins是一个开源的、提供友好操作界面的持续集成(CI)工具，起源于Hudson（Hudson是商用的），主要用于持续、自动的构建/测试软件项目、监控外部任务的运行（这个比较抽象，暂且写上，不做解释）。Jenkins用Java语言编写，可在Tomcat等流行的servlet容器中运行，也可独立运行。通常与版本管理工具(SCM)、构建工具结合使用。常用的版本控制工具有SVN、GIT，构建工具有Maven、Ant、Gradle。 使用目的： 快速部署爬虫项目到scrapyd，并直接启动爬虫。 2、使用 1.打开jenkins 地址：http://192.168.0.11:8088/，账号为姓名全拼，密码123456，主界面如下 2.新建任务 点击左上方的“新建任务”，输入任务名称，任务名称需要使用“Scrapy_”开头。一个行业一个jenkins 项目，所以后面的名字最好可以标志该行业，例如：“Scrapy_C16_data”,代表C16行业的数据爬虫的 jenkins项目。然后选择“构建一个自由风格的软件项目” 3.项目设置 在这里要对构建过程自定义。先设置通用功能 注意：名称填\"SPIDER_ITEM\"， 默认值中写行业工程中工程名以及工程中的爬虫名，格式： \"项目名称:爬虫名1,爬虫名2\"，多个项目需要分行写。描述选项是对这个参数的说明。 这样写jenkins在构建的时候会先上传“项目”，然后再启动项目中的“爬虫”，如果单单只想上传“项目 的话”， 把“项目名称”后面从冒号开始删掉就行了。例如直接写个“项目名称”，就只会上传这个项 目。 选项描述：“爬虫项目列表，格式 “爬虫项目名: 爬虫名1, 爬虫名2........”, 多个爬虫项目分行填写 例如： Scrapy_baidu: bai1, bai2 Scrapy_souhu: sou1, sou2, sou3” 写以上这个描述就好。 接下来设置git地址： 接下来设置构建后操作： 选择“Send build artifacts over SSH”选项： 脚本： data=\"${SPIDER_ITEM}\" sh /app/spider_build.sh -j ${JOB_BASE_NAME} -u '自己的scrapyd地址' -d \"$data\" 改下scrpyd的地址，直接粘贴进去就ok。 然后保存。回到首页，点击我们刚才新建的Scrapy_C16_data. 点击“build with param” 进入如下界面： 可以看到我们直接新建的变量。大家可以自己选择要构建哪个项目，启动哪个爬虫。自己进行删改就行。选好了，点击“开始构建”。 等待进度条走完，看下前面的图标是什么颜色。。。蓝色代表构建成功，其他颜色一律代表失败。如果失败那么点击那个 “#56”进去，可以查看日志。 可以在里面看到构建信息。如果有错误，那么就进行错误排查。 Jenkins教程如上。不清楚可以再问。 可能出现的错误 一． Unknown target: demo 因为设置的部署命令是scrapyd-deploy demo -p “爬虫工程名” 默认是demo，所以在scrapyd.cfg文件中的配置最好写成demo 二． 这种错误是由于shell脚本执行时间过长，导致jenkins超时，解决方法就是： 把Exec timeout选项参数设置大一点，默认2分钟超时，可以设置成240000ms（4分钟）超时。 三． 出现 spider ‘xxxx’ not found的错误，这是由于爬虫名字写错造成的。请严格注意大小写，空格等，务必与爬虫文件中的name属性保持一致。 "},"part3_tech/git.html":{"url":"part3_tech/git.html","title":"git操作","keywords":"","body":"git 一、git 描述&安装初始化 git是版本管理工具，一般可用于gitlab、github、码云（gitee）等代码管理网站 git 分为三个区：工作区，暂存区和版本库 工作区 一般指本地磁盘，拥有.git（隐藏文件夹）的文件夹 暂存区 在版本库和工作区之间，相当于文件中转站 版本库 ​ 就是仓库,分为本地仓库和远程仓库（gitlab等） git下载：https://github.com/waylau/git-for-win/ 安装一直next就行 刚下载完git 配置下用户名 打开git命令行输入以下命令 git config --global user.name \"your name\" git config --global user.email \"your email\" 查看用户信息 git config -l 可以看见用户刚配置的用户和系统信息 二、Git指令 生成本地ssh 进入Administrator的.ssh文件夹下，运行git-bash, 命令 ssh-keygen -t rsa -C \"your email\" 输入该ssh的名字 输入密码（可不写） 将.pub文件内容粘贴到你的gitlab上 这样你就可以使用ssh的url进行上传文件了 基本指令 指令 解释 git config --global user.name \"your name\" 初始化用户名 git config --global user.email\"your email\" 初始化邮箱 git init 创建一个本地git仓库 git add . 添加修改的提交到暂存区 git commit -m\"commit message\" 提交并注释 git remote add [name] [url.git] 添加远程仓库地址 git push 本地推送至远程 git pull/fetch 远程下载到本地 git clone [url.git] 下载到本地 git log 查看提交日志 git status 查看仓库状态 git remote add [远程名] [git地址] 将本地关联到远程 git remote [-v] 查看本地关联的远程名，可以关联多个远程 git remote rm [远程名] 删除关联的远程 http://192.168.0.11/b_07/scrapy_zhengce_v1_07.git git@192.168.0.11:b_07/scrapy_zhengce_v1_07.git 回滚撤销 请戳此处 分支指令 创建分支 git branch [b-name] 切换分支 git checkout [b-name] 分支合并 git merge 列出分支 git branch 删除分支 git branch -d [b-name] 请戳此处 三、工作提交 两种方法，建议使用第一种方法 1、 创建一个gitlab的仓库项目，复制http的git地址 git clone [url.git] 下载到本地 进入到刚下载的git 文件夹， 把你要上传的文件，复制到此文件夹，修改README.md文件 git add . ----> git commit -m\"your commit message\"----> git push 2、 建立一个远程仓库 创建一个空文件夹，使用git init 变成本地git库 添加远程仓库地址 git remote add [origin] [git地址] 先使用git pull origin master master,将远程推送至本地 将你要上传的文件放入此git文件夹 git push origin master上传到远程仓库 以后还有补充，大家先用着这么多 "},"part3_tech/xxl_clean.html":{"url":"part3_tech/xxl_clean.html","title":"定时数据上传接口文档","keywords":"","body":"xxl-job数据定时插入 简介：资讯，数据，报告定时更新到数据库 HOST:192.168.0.11:8066 联系人: Version:1.0 接口路径：/v2/api-docs 数据导入xxl-job数据库 把行业数据插入xxl的数据库 接口描述:把行业数据插入xxl的数据库 接口地址:/quota/data/insert 请求方式：POST consumes:[\"application/json\"] produces:[\"*/*\"] 请求示例： [ { \"country\": \"\", \"data_day\": 0, \"data_month\": 0, \"data_source\": \"\", \"data_value\": 0, \"data_year\": 0, \"frequency\": 0, \"indic_name\": \"\", \"path\": [], \"region\": \"\", \"sign\": \"\", \"status\": 0, \"unit\": \"\" } ] 请求参数： 参数名称 参数说明 in 是否必须 数据类型 schema mons mons body true array QuotaMongoDTO schema属性说明 QuotaMongoDTO 参数名称 参数说明 in 是否必须 数据类型 schema country body false string data_day body false integer(int32) data_month body false integer(int32) data_source body false string data_value 数据内容 body true number(double) data_year body false integer(int32) frequency 数据频率 body true integer(int32) indic_name 数据名字 body true string path 数据的菜单列表，从行业大类开始至最后一级菜单为止 body true array region body false string sign body false string status body false integer(int32) unit 单位 body false string 响应示例: { \"code\": 0, \"data\": {}, \"msg\": \"\", \"success\": true } 响应参数: 参数名称 参数说明 类型 schema code integer(int32) integer(int32) data object msg string success boolean 响应状态: 状态码 说明 schema 200 OK ResponseDTO 201 Created 401 Unauthorized 403 Forbidden 404 Not Found 把行业报告插入xxl的数据库 接口描述:把行业报告插入xxl的数据库 接口地址:/report/data/insert 请求方式：POST consumes:[\"application/json\"] produces:[\"*/*\"] 请求示例： [ { \"author\": \"\", \"day\": 0, \"image_url\": \"\", \"month\": 0, \"paper\": \"\", \"paper_abstract\": \"\", \"paper_from\": \"\", \"paper_url\": \"\", \"path\": [], \"title\": \"\", \"year\": 0 } ] 请求参数： 参数名称 参数说明 in 是否必须 数据类型 schema reports reports body true array ReportMongoDTO schema属性说明 ReportMongoDTO 参数名称 参数说明 in 是否必须 数据类型 schema author 报告pdf作者 body false string day 日 body false integer(int32) image_url 标题图片的url路径 body false string month 月 body false integer(int32) paper 报告pdf文件的存放路径 body true string paper_abstract 报告pdf内容简介 body false string paper_from 报告pdf来源 body false string paper_url 报告pdf文件的网站来源url body false string path 报告pdf文件的菜单，从行业大类开始 body true array title 报告标题 body true string year 年 body false integer(int32) 响应示例: { \"code\": 0, \"data\": {}, \"msg\": \"\", \"success\": true } 响应参数: 参数名称 参数说明 类型 schema code integer(int32) integer(int32) data object msg string success boolean 响应状态: 状态码 说明 schema 200 OK ResponseDTO 201 Created 401 Unauthorized 403 Forbidden 404 Not Found 资讯导入xxl-job数据库 向mongo中导入资讯 接口描述:向mongo中导入资讯 接口地址:/policy/insertToMongo 请求方式：POST consumes:[\"application/json\"] produces:[\"*/*\"] 请求示例： [ { \"content\": \"\", \"content_url\": \"\", \"day\": 0, \"information_source\": \"\", \"month\": 0, \"path\": [], \"tags\": \"\", \"title\": \"\", \"title_image\": \"\", \"year\": 0 } ] 请求参数： 参数名称 参数说明 in 是否必须 数据类型 schema policy policy body true array PolicyRecMongoDTO schema属性说明 PolicyRecMongoDTO 参数名称 参数说明 in 是否必须 数据类型 schema content 资讯内容 body true string content_url 文章的链接 body false string day body false integer(int32) information_source 文章的来源网站的名称 body false string month body false integer(int32) path 报告pdf文件的菜单，从行业大类开始 body true array tags 文章的分类标签 body false string title 文章的标题 body true string title_image 文章的标题图片 body false string year body false integer(int32) 响应示例: { \"code\": 0, \"data\": {}, \"msg\": \"\", \"success\": true } 响应参数: 参数名称 参数说明 类型 schema code integer(int32) integer(int32) data object msg string success boolean 响应状态: 状态码 说明 schema 200 OK ResponseDTO 201 Created 401 Unauthorized 403 Forbidden 404 Not Found "},"part4_scrapy/":{"url":"part4_scrapy/","title":"scrapy相关","keywords":"","body":"scrapy相关 .... "},"part4_scrapy/scrapy_settings.html":{"url":"part4_scrapy/scrapy_settings.html","title":"scrapy设置文件","keywords":"","body":"配置文件 目录 1. 常用配置 2. settings.py配置(总)) 1. 常用配置 #==>第一部分：基本配置第二部分：并发与延迟第三部分：智能限速/自动节流：AutoThrottle extension第四部分：爬取深度与爬取方式第五部分：中间件、Pipelines、扩展第六部分：缓存 2. settings.py配置(总) settings.py中配置及注释： # 项目名称 BOT_NAME = '$project_name' SPIDER_MODULES = ['$project_name.spiders'] NEWSPIDER_MODULE = '$project_name.spiders' # 在项目处理器（也称为“ 项目管道”）中并行处理的最大并发项目数（每个响应），默认100。 #CONCURRENT_ITEMS = 100 # Scrapy下载器将执行的并发（即，并发）请求的最大数量，默认16 CONCURRENT_REQUESTS = 8 # 从同一网站下载连续页面之前，下载程序应等待的时间（以秒为单位）。 # 这可以用来限制爬网速度，以避免对服务器造成太大的冲击。支持小数。 # 默认情况下，Scrapy不会在请求之间等待固定的时间，而是使用0.5 * DOWNLOAD_DELAY和1.5 * DOWNLOAD_DELAY之间的随机间隔。 #DOWNLOAD_DELAY = 0 # 将对任何单个域执行的并发（即，并发）请求的最大数量，默认8 #CONCURRENT_REQUESTS_PER_DOMAIN = 16 # 将对任何单个IP执行的并发（即，并发）请求的最大数量，默认0。 # 如果非0，CONCURRENT_REQUESTS_PER_DOMAIN这个参数会被忽略，即按IP不按域名。DOWNLOAD_DELAY也是按IP #CONCURRENT_REQUESTS_PER_IP = 16 # 将用于实例化Scrapy shell中的项目的默认类 #DEFAULT_ITEM_CLASS = 'scrapy.item.Item' # 对于任何站点，将允许爬网的最大深度。如果为零，则不施加限制 #DEPTH_LIMIT = 0 # 根据DEPTH_PRIORITY的值取决于深度优先或广度优先，即正值为广度优先(BFO)，负值为深度优先(DFO) # 计算公式：request.priority = request.priority - ( depth * DEPTH_PRIORITY ) #DEPTH_PRIORITY = 0 # 是否启用cookie COOKIES_ENABLED = False # 如果启用，Scrapy将记录请求中发送的所有cookie（即Cookie 标头）和响应中接收的所有cookie（即Set-Cookie标头） #COOKIES_DEBUG = False # 是否收集详细的深度统计信息。如果启用此功能，则在统计信息中收集每个深度的请求数 #DEPTH_STATS_VERBOSE = False # 是否启用DNS内存缓存 #DNSCACHE_ENABLED = True # DNS内存缓存大小 #DNSCACHE_SIZE = 10000 # 处理DNS查询的超时时间（以秒为单位）。支持浮动 #DNS_TIMEOUT = 60 # 用于爬网的下载器 #DOWNLOADER = 'scrapy.core.downloader.Downloader' # Disable Telnet Console (enabled by default) #TELNETCONSOLE_ENABLED = False # 包含您的项目中启用的下载器中间件及其命令的字典 #DOWNLOADER_MIDDLEWARE = {} # 用于Scrapy HTTP请求的默认标头。它们被填充在 DefaultHeadersMiddleware DEFAULT_REQUEST_HEADERS = { } # Scrapy中默认启用的下载程序中间件的字典。低值更接近引擎，高值更接近下载器， # 不要试图修改此设置，请修改DOWNLOADER_MIDDLEWARE #DOWNLOADER_MIDDLEWARES_BASE = { # 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100, # 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300, # 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350, # 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400, # 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500, # 'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550, # 'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560, # 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580, # 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590, # 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600, # 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700, # 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750, # 'scrapy.downloadermiddlewares.stats.DownloaderStats': 850, # 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900, # } # 是否启用下载器统计信息收集 #DOWNLOADER_STATS = True # 包含在项目中启用的请求下载处理程序的字典 #DOWNLOAD_HANDLERS = {} # 包含请求下载处理程序的默认字典 # 如果要禁用FTP处理程序，请设置DOWNLOAD_HANDLERS = {'ftp': None} #DOWNLOAD_HANDLERS_BASE = { # 'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler', # 'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler', # 'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler', # 's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler', # 'ftp': 'scrapy.core.downloader.handlers.ftp.FTPDownloadHandler', # } # 下载程序的超时时间（以秒为单位） #DOWNLOAD_TIMEOUT = 180 # 载程序将下载的最大响应大小（以字节为单位,默认1024MB），为0则不限制 #DOWNLOAD_MAXSIZE = 1073741824 # 下载程序将开始警告的响应大小（以字节为单位，默认32MB） #DOWNLOAD_WARNSIZE = 33554432 # 声明的Content-Length与服务器发送的内容不匹配，是否触发异常ResponseFailed([_DataLoss]) # 如果为False，可以在爬虫文件中判断并处理 if 'dataloss' in response.flags: #DOWNLOAD_FAIL_ON_DATALOSS = True # 用于检测和过滤重复请求的类 #DUPEFILTER_CLASS = 'scrapy.dupefilters.RFPDupeFilter' # 默认情况下，RFPDupeFilter仅记录第一个重复的请求。设置DUPEFILTER_DEBUG为True它将记录所有重复的请求。 #DUPEFILTER_DEBUG = False # 包含您的项目中启用的扩展及其顺序的字典 #EXTENSIONS = {} # 包含默认情况下在Scrapy中可用的扩展程序及其顺序的字典 #EXTENSIONS_BASE = { # 'scrapy.extensions.corestats.CoreStats': 0, # 'scrapy.extensions.telnet.TelnetConsole': 0, # 'scrapy.extensions.memusage.MemoryUsage': 0, # 'scrapy.extensions.memdebug.MemoryDebugger': 0, # 'scrapy.extensions.closespider.CloseSpider': 0, # 'scrapy.extensions.feedexport.FeedExporter': 0, # 'scrapy.extensions.logstats.LogStats': 0, # 'scrapy.extensions.spiderstate.SpiderState': 0, # 'scrapy.extensions.throttle.AutoThrottle': 0, # } # 包含要使用的项目管道及其顺序的字典。值是任意的，但是习惯上将它们定义在0-1000范围内。低值优先于高值 #ITEM_PIPELINES = {} # 是否启用日志记录 #LOG_ENABLED = True # 用于日志记录的编码 #LOG_ENCODING = 'utf-8' # 用于记录输出的文件名 #LOG_FILE = None # 用于格式化日志消息的字符串 #LOG_FORMAT = '%(asctime)s [%(name)s] %(levelname)s: %(message)s' # 用于格式化日期/时间的字符串，用于改变LOG_FORMAT 中的asctime占位符 #LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S' # 用于格式化不同操作的日志消息的类 #LOG_FORMATTER = \"scrapy.logformatter.LogFormatter\" # 最低记录级别, 可用：CRITICAL, ERROR, WARNING, INFO, DEBUG #LOG_LEVEL = 'DEBUG' # 如果为True，所有标准输出（和错误）将被重定向到日志，例如print也会被记录在日志 #LOG_STDOUT = False # 如果为True，则日志将仅包含根路径;如果设置为False，则显示负责日志输出的组件 #LOG_SHORT_NAMES = False # 每次统计记录打印输出之间的间隔（以秒为单位） #LOGSTATS_INTERVAL = 60.0 # 是否启用内存调试 #MEMDEBUG_ENABLED = False # 启用内存调试后，如果此设置不为空，则会将内存报告发送到指定的邮箱地址，否则该报告将被写入日志。 # 例如：MEMDEBUG_NOTIFY = ['user@example.com'] #MEMDEBUG_NOTIFY = [] # 是否启用内存使用扩展。此扩展跟踪该进程使用的峰值内存（将其写入统计信息）。 # 当超过内存限制时，它还可以选择关闭Scrapy进程，并在发生这种情况时通过电子邮件通知 #MEMUSAGE_ENABLED = True # 关闭Scrapy之前允许的最大内存量 #MEMUSAGE_LIMIT_MB = 0 #MEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0 # 电子邮件列表，用于通知是否已达到内存限制 #MEMUSAGE_NOTIFY_MAIL = False # 发送警告电子邮件通知最大内存之前允许的最大内存量（以兆字节为单位）。如果为零，则不会发出警告 #MEMUSAGE_WARNING_MB = 0 # 使用genspider命令创建爬虫的模板 #NEWSPIDER_MODULE = \"\" # 如果启用，Scrapy将在从同一网站获取请求的同时等待随机的时间（介于0.5 * DOWNLOAD_DELAY和1.5 *之间DOWNLOAD_DELAY） #RANDOMIZE_DOWNLOAD_DELAY = True # Twisted Reactor线程池大小的最大限制。这是各种Scrapy组件使用的通用多用途线程池。 # 线程DNS解析器，BlockingFeedStorage，S3FilesStore仅举几例。 # 如果遇到阻塞IO不足的问题，请增加此值。 #REACTOR_THREADPOOL_MAXSIZE = 10 # 定义可以重定向请求的最长时间。超过此最大值后，将按原样返回请求的响应 #REDIRECT_MAX_TIMES = 20 # 调整重定向请求的优先级，为正则优先级高 #REDIRECT_PRIORITY_ADJUST = 2 # 调整重试请求的优先级 #RETRY_PRIORITY_ADJUST = -1 # 是否遵循robot协议 ROBOTSTXT_OBEY = False # 用于解析robots.txt文件的解析器后端 #ROBOTSTXT_PARSER = 'scrapy.robotstxt.ProtegoRobotParser' #ROBOTSTXT_USER_AGENT = None # 用于爬网的调度程序 #SCHEDULER = 'scrapy.core.scheduler.Scheduler' # 设置为True将记录有关请求调度程序的调试信息 #SCHEDULER_DEBUG = False # 调度程序将使用的磁盘队列的类型。其他可用类型：scrapy.squeues.PickleFifoDiskQueue， # scrapy.squeues.MarshalFifoDiskQueue， scrapy.squeues.MarshalLifoDiskQueue #SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue' # 调度程序使用的内存队列的类型。其他可用类型： scrapy.squeues.FifoMemoryQueue #SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue' # 调度程序使用的优先级队列的类型。另一种可用的类型是 scrapy.pqueues.DownloaderAwarePriorityQueue #SCHEDULER_PRIORITY_QUEUE = 'scrapy.pqueues.ScrapyPriorityQueue' # 正在处理响应数据的软限制（以字节为单位）。 # 如果所有正在处理的响应的大小总和高于此值，Scrapy不会处理新的请求 #SCRAPER_SLOT_MAX_ACTIVE_SIZE = 5_000_000 # 包含您的项目中启用的蜘蛛合约的字典，用于测试蜘蛛 #SPIDER_CONTRACTS = {} # 包含Scrapy合同中默认启用的Scrapy合同的字典 #SPIDER_CONTRACTS_BASE = { # 'scrapy.contracts.default.UrlContract' : 1, # 'scrapy.contracts.default.ReturnsContract': 2, # 'scrapy.contracts.default.ScrapesContract': 3, # } # 将用于加载蜘蛛的类 #SPIDER_LOADER_CLASS = 'scrapy.spiderloader.SpiderLoader' # 包含您的项目中启用的蜘蛛中间件及其命令的字典 #SPIDER_MIDDLEWARES = {} #SPIDER_MIDDLEWARES_BASE = { # 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware': 50, # 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': 500, # 'scrapy.spidermiddlewares.referer.RefererMiddleware': 700, # 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware': 800, # 'scrapy.spidermiddlewares.depth.DepthMiddleware': 900, # } # Scrapy将在其中寻找蜘蛛的模板列表 #SPIDER_MODULES = {} # 用于收集统计信息的类 #STATS_CLASS = 'scrapy.statscollectors.MemoryStatsCollector' # 蜘蛛完成后，将Scrapy统计信息转储到Scrapy日志中 #STATS_DUMP = True # 蜘蛛抓取完毕后发送Scrapy统计信息的邮箱列表 #STATSMAILER_RCPTS = [] # 指定是否 将启用telnet控制台 #TELNETCONSOLE_ENABLED = True # 用于telnet控制台的端口范围。如果设置为None或0，则使用动态分配的端口 #TELNETCONSOLE_PORT = [6023, 6073] # 使用startproject命令创建新项目和使用 genspider命令创建新的Spider时要在其中查找模板的目录 #TEMPLATES_DIR = \"templates\" # 允许抓取的URL的最大URL长度 #URLLENGTH_LIMIT = 2083 # 爬网时使用的默认User-Agent #USER_AGENT = \"Scrapy/VERSION (+https://scrapy.org)\" "},"part4_scrapy/scrapyd.html":{"url":"part4_scrapy/scrapyd.html","title":"scrapyd配置","keywords":"","body":"scrapyd部署(window端) 介绍 需要安装以下库 pip install [库名] scrapyd：服务器端 scrapyd-client：客户端 scrapyd-deploy：一个工具，打包egg并且自动化部署到服务器 部署步骤 参考于简书（https://www.jianshu.com/p/ddd28f8b47fb） 打开服务端端：命令行运行scrapyd,配置成功会生成dbs和eggs文件夹 在项目文件夹下，修改scrapy.cfg配置文件，取消url的注释 在项目文件夹下，测试scrapyd-deploy（windows），参考以上网址修改 将项目部署到服务器scrapyd-deploy [deploy名] -p [项目名] 启动项目：curl http://localhost:6800/schedule.json -d project=[项目名] -d spider=[spider名] scrapy.cfg的修改 [deploy:demo] url = http://192.168.0.11:6800/addversion.json 部署运行 scrapyd-deploy demo -p [项目名] # 部署到服务器 # 运行spider curl http://192.168.0.11:6800/schedule.json -d project=[] -d spider=[] 操作 解释 scrapyd-deploy [deploy名] -p [项目名] 将项目部署到服务器 curl http://localhost:6800/schedule.json -d project=[项目名] -d spider=[spider名] 启动项目 curl http://localhost:6800/daemonstatus.json 查看服务端状态 curl http://192.168.0.11:6800/listprojects.json 列出上传的项目列表 curl http://192.168.1.9:6800/listversions.json\\?project\\=[项目名] 列出有某个上传项目的版本 curl http://192.168.1.9:6800/cancel.json -d project=[项目名]-d job=[job id] 取消爬虫运行 "},"part4_scrapy/scrapy-note.html":{"url":"part4_scrapy/scrapy-note.html","title":"scrapy笔记","keywords":"","body":"scrapy 笔记 目录 一、中间件 1.1 scrapy中间件的默认优先级 1.2 retry中间件 1.3 代理中间件自定义 1.4 添加selenium 二、其它 2.1 scrapy log日志 2.2 scrapy 执行 2.3 scrapy-redis 2.4 媒体文件下载 2.5 捕获异常 2.6 添加cookies三种方法 2.7 发送json数据(post)) 2.8 cookiejar使用 2.9 scrapy shell 加入ua和cookie 2.10 优雅的导入settings的配置 声明 一、中间件 1.1 scrapy中间件的默认优先级 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100, 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300, 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350, 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500, 'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550, 'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560, 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580, 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590, 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600, 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700, 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750, 'scrapy.downloadermiddlewares.stats.DownloaderStats': 850, 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900, process_request() 每个中间件的方法将以递增的中间件顺序（100、200、300，...）process_response()每个中间件的方法将以递减(300,200,100)顺序被调用 1.2 retry中间件 代码1:保存信息 class GetFailedUrl(RetryMiddleware): def __init__(self, settings): self.max_retry_times = settings.getint('RETRY_TIMES') self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES')) self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST') def process_response(self, request, response, spider): if response.status in self.retry_http_codes: # 将爬取失败的URL存下来，你也可以存到别的存储 with open(str(spider.name) + \".txt\", \"a\") as f: f.write(response.url + \"\\n\") return response return response def process_exception(self, request, exception, spider): # 出现异常的处理 if isinstance(exception, self.EXCEPTIONS_TO_RETRY) \\ and not request.meta.get('dont_retry', False): with open(str(spider.name) + \".txt\", \"a\") as f: f.write(str(request) + \"\\n\") return None 代码2：重试 class MyRetryMiddleware(RetryMiddleware): def process_response(self, request, response, spider): if request.meta.get('dont_retry', False): return response if response.status in self.retry_http_codes: reason = response_status_message(response.status) # 删除该代理 time.sleep(random.randint(3, 5)) self.logger.warning('返回值异常, 进行重试...') return self._retry(request, reason, spider) or response return response def process_exception(self, request, exception, spider): if isinstance(exception, self.EXCEPTIONS_TO_RETRY) \\ and not request.meta.get('dont_retry', False): # 删除该代理 time.sleep(random.randint(3, 5)) self.logger.warning('连接异常, 保存文件后进行重试...') with open(str(spider.name) + \".txt\", \"a\") as f: f.write(str(request) + \"\\n\") return self._retry(request, exception, spider) 1.3 代理中间件自定义 代码 class Proxy(): def __init__(self): self.url = 'http://192.168.0.11:5010/get/' self.del_url = \"http://192.168.0.11:5010/delete?proxy={}\" # 网站是http的就改成http，网站是https的就改成https self.proxy_header = 'http://' def get_proxy(self): proxy = self.proxy_header + requests.get(self.url).json().get('proxy',False) return proxy def del_proxy(self, proxy): # 删除代理池的ip，慎用 requests.get(self.del_url.format(proxy.lstrip(self.proxy_header))) class AddProxyMiddlewares(): def __init__(self): # 失效代理的集合 self.invalid_proxy = set() self.proxy = Proxy().get_proxy() def process_request(self,request,spider): request.meta['HTTP_USER_AGENT'] = UserAgent().chrome # 清空无效代理 if len(self.invalid_proxy) > 10: self.invalid_proxy.clear() request.meta[\"proxy\"] = self.proxy logger.info('now proxy is {}'.format(self.proxy)) return None def process_response(self,request,response,spider): if str(response.status).startswith('4') or str(response.status).startswith('5'): logger.warning('bad response.status {},'.format(str(response.status))) # 获取失效代理并收集,重新获取一个代理 NgProxy = request.meta['proxy'] self.invalid_proxy.add(NgProxy) logger.info('it is process_response invalid proxy is {},'.format(NgProxy)) self.proxy = Proxy().get_proxy() return response def process_exception(self,request,exception,spider): # 进入异常模块， NgProxy = request.meta['proxy'] logger.error('spider is exception, exception infomation is %s', exception) logger.error('this is in process_exception, invalid proxy is {},'.format(NgProxy)) self.invalid_proxy.add(NgProxy) 1.4 添加selenium from selenium import webdriver from selenium.common.exceptions import TimeoutException from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from scrapy.http import HtmlResponse logger = logging.getLogger(__name__) class SeleniumMiddle(): def __init__(self, timeout=10, service_args=[]): self.timeout = timeout option = webdriver.FirefoxOptions() option.add_argument('-headless') # 设置option,后台运行 self.browser = webdriver.Firefox(firefox_options=option) # self.browser.set_window_size(1400, 700) self.browser.set_page_load_timeout(self.timeout) self.wait = WebDriverWait(self.browser, self.timeout) def __del__(self): self.browser.close() def process_request(self, request, spider): \"\"\" 抓取页面 :param request: Request对象 :param spider: Spider对象 :return: HtmlResponse \"\"\" logger.debug('Firefox is Starting') try: self.browser.get(request.url) self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#test_table'))) page_source = self.browser.page_source return HtmlResponse(url=request.url, body=page_source, request=request, encoding='utf-8', status=200) except TimeoutException: return HtmlResponse(url=request.url, status=501, request=request) 二、其它 2.1 scrapy log日志 # 日志文件settings.py中 LOG_LEVEL= 'DEBUG' LOG_FILE ='log.txt' 爬虫代码中 可以直接引用self.logger self.logger.info(...) 但是middleware中不能直接引用,需要导入模块 默认time_out时间为180s 修改: request.meta['download_timeout']=60 (数值自己定) 2.2 scrapy 执行 第一种，简单，但类本身在最后一行有句sys.exit(cmd.exitcode)，注定了他执行完就退出程序，不再执行后面的语句，所以只适合调试时使用。 # -*- coding:utf-8 -*- from scrapy import cmdline # 方式一：注意execute的参数类型为一个列表 cmdline.execute('scrapy crawl spidername'.split()) # 方式二:注意execute的参数类型为一个列表 cmdline.execute(['scrapy', 'crawl', 'spidername']) 第二种，相对第一种会多几行代码，但是没有第一种的缺点，建议使用。 from scrapy.crawler import CrawlerProcess from scrapy.utils.project import get_project_settings process = CrawlerProcess(get_project_settings()) # 'followall' is the name of one of the spiders of the project. process.crawl('SpiderName', domain='123.com') process.start() # the script will block here until the crawling is finished 2.3 scrapy-redis scrapy_redis lpush [键名]:start_urls http://youjia.chemcp.com/youjiamap.asp 添加入redis数据库 来启动redis settings.py中设置 # 启动scrapy-redis引擎 SCHEDULER = \"scrapy_redis.scheduler.Scheduler\" # 共享筛选器 DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\" # 不清空爬虫队列 SCHEDULER_PERSIST = True # 使用优先级队列调度 SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue' 'scrapy_redis.pipelines.RedisPipeline': 300, REDIE_URL = None REDIS_HOST = '127.0.0.1' REDIS_PORT = 6379 2.4 媒体文件下载 file示例 settings.py # 文件下载路径 FILES_STORE = 'D:\\result' pipeline class MyFilePipeline(FilesPipeline): def get_media_requests(self, item,info): yield scrapy.Request(item['content_url']) def file_path(self, request, response=None, info=None): \"\"\" 重命名模块 \"\"\" path = os.path.join(FILES_STORE, request.url[-10:]) logger.info(\"file_path:{}\".format(request.url[-10:])) return path 2.5 捕获异常 from twisted.internet import defer from twisted.internet.error import TimeoutError, DNSLookupError, \\ ConnectionRefusedError, ConnectionDone, ConnectError, \\ ConnectionLost, TCPTimedOutError from scrapy.http import HtmlResponse from twisted.web.client import ResponseFailed from scrapy.core.downloader.handlers.http11 import TunnelError class ProcessAllExceptionMiddleware(object): ALL_EXCEPTIONS = (defer.TimeoutError, TimeoutError, DNSLookupError, ConnectionRefusedError, ConnectionDone, ConnectError, ConnectionLost, TCPTimedOutError, ResponseFailed, IOError, TunnelError) def process_response(self,request,response,spider): #捕获状态码为40x/50x的response if str(response.status).startswith('4') or str(response.status).startswith('5'): #随意封装，直接返回response，spider代码中根据url==''来处理response response = HtmlResponse(url='') return response #其他状态码不处理 return response def process_exception(self,request,exception,spider): #捕获几乎所有的异常 if isinstance(exception, self.ALL_EXCEPTIONS): #在日志中打印异常类型 print('Got exception: %s' % (exception)) #随意封装一个response，返回给spider response = HtmlResponse(url='exception') return response #打印出未捕获到的异常 print('not contained exception: %s'%exception) 2.6 添加cookies三种方法 1.settings settings文件中给Cookies_enabled=False解注释 settings的headers配置的cookie就可以用了 这种方法最简单，同时cookie可以直接粘贴浏览器的。 后两种方法添加的cookie是字典格式的，需要用json反序列化一下, 而且需要设置settings中的Cookies_enabled=True 2.DownloadMiddleware settings中给downloadmiddleware解注释 去中间件文件中找downloadmiddleware这个类，修改process_request，添加request.cookies={}即可。 3.爬虫主文件中重写start_request def start_requests(self): yield scrapy.Request(url,dont_filter=True,cookies={自己的cookie}) 2.7 发送json数据(post) scrapy Post 发送数据是我们通常会用 yield scrapy.FormRequest( url = url, formdata = {\"email\" : \"xxx\", \"password\" : \"xxxxx\"}, callback = self.parse_page ) 来发送请求，但这是发送header为 'Content-Type', 'application/x-www-form-urlencoded' 的数据，有时候我们做一些爬虫，会post发送json数据，否则不能返回正确的结果，如果直接用request很方便，但在scrapy里就需要这样用 yield Request(url, method=\"POST\", body=json.dumps(data), headers={'Content-Type': 'application/json'},callback=self.parse_json) 这样就可以发送json 数据了 2.8 cookiejar使用 meta当然是可以传递cookie的（第一种）： 下面start_requests中键‘cookiejar’是一个特殊的键，scrapy在meta中见到此键后，会自动将cookie传递到要callback的函数中。既然是键(key)，就需要有值(value)与之对应，例子中给了数字1，也可以是其他值，比如任意一个字符串。 def start_requests(self): yield Request(url,meta={'cookiejar':1},callback=self.parse) 需要说明的是，meta给‘cookiejar’赋值除了可以表明要把cookie传递下去，还可以对cookie做标记。一个cookie表示一个会话(session)，如果需要经多个会话对某网站进行爬取，可以对cookie做标记，1,2,3,4......这样scrapy就维持了多个会话。 def parse(self,response): key=response.meta['cookiejar'] #经过此操作后，key=1 yield Request(url2,meta={'cookiejar'：key},callback='parse2') def parse2(self,response): pass 上面这段和下面这段是等效的： def parse(self,response): yield Request(url2,meta={'cookiejar'：response.meta['cookiejar']},callback='parse2') #这样cookiejar的标记符还是数字1 def parse2(self,response): pass 传递cookie的第二种写法： 如果不加标记，可以用下面的写法： #先引入CookieJar()方法 from scrapy.http.cookies import CookieJar 写spider方法时： def start_requests(self): yield Request(url,callback=self.parse)#此处写self.parse或‘parse’都可以 def parse(self,response): cj = response.meta.setdefault('cookie_jar', CookieJar()) cj.extract_cookies(response, response.request) container = cj._cookies yield Request(url2,cookies=container,meta={'key':container},callback='parse2') def parse2(self,response): pass meta是浅复制，必要时需要深复制。 可以这样引入： import copy meta={'key':copy.deepcopy('value')} 2.9 scrapy shell 加入ua和cookie scrapy shell 'https://www.qq.com/' -s USER_AGENT='(Windows NT 10.0; Win64; x64; rv:67.0) Gecko/20100101 Firefox/67.0' # 指定请求目标的 URL 链接 url_ = 'https://www.google.com' # 自定义 Headers 请求头(一般建议在调试时使用自定义 UA，以绕过最基础的 User-Agent 检测) headers_ = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134'} # 构造需要附带的 Cookies 字典 cookies_ = {\"key_1\": \"value_1\", \"key_2\": \"value_2\", \"key_3\": \"value_3\"} # 构造 Request 请求对象 req = scrapy.Request(url, cookies=cookies, headers=headers_) # 发起 Request 请求 fetch(req) # 在系统默认浏览器查看请求的页面（主要为了检查是否正常爬取到内页） view(response) 2.10 优雅的导入settings的配置 scrapy提供了导入设置的方法：from_crawler @classmethod def from_crawler(cls, crawler): server = crawler.settings.get('SERVER') # FIXME: for now, stats are only supported from this constructor return cls(server) 接着，只要在init接收这些参数就可以了。 def __init__(self, server): self.server = server 而在一些官方的组件的源码中会这样使用，不过这看起来有点多此一举 @classmethod def from_settings(cls, settings): server = settings.get('SERVER') return cls(server) @classmethod def from_crawler(cls, crawler): # FIXME: for now, stats are only supported from this constructor return cls.from_settings(crawler.settings) 另外，并不是所有的类都可以使用这个类方法。只有像插件,中间件,信号管理器和项目管道等这些组件才能使用这个类方法来导入配置，如果是自己写的spider或者自定义文件并没有，需要使用如下方法导入： from scrapy.utils.project import get_project_settings settings = get_project_settings() 这里的settings就是包含settings.py的所有配置的字典了。 声明 内容基本上来源于网络，仅提供参考作用，需自己验证 "}}